{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b638e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pythonnet\n",
      "  Downloading pythonnet-3.0.1-py3-none-any.whl (284 kB)\n",
      "     ------------------------------------ 284.5/284.5 kB 247.3 kB/s eta 0:00:00\n",
      "Collecting clr-loader<0.3.0,>=0.2.2\n",
      "  Downloading clr_loader-0.2.5-py3-none-any.whl (51 kB)\n",
      "     -------------------------------------- 51.1/51.1 kB 869.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: cffi>=1.13 in c:\\users\\anann\\anaconda3\\lib\\site-packages (from clr-loader<0.3.0,>=0.2.2->pythonnet) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\anann\\anaconda3\\lib\\site-packages (from cffi>=1.13->clr-loader<0.3.0,>=0.2.2->pythonnet) (2.21)\n",
      "Installing collected packages: clr-loader, pythonnet\n",
      "Successfully installed clr-loader-0.2.5 pythonnet-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pythonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e60d73ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import system\n",
    "\n",
    "import librosa\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a10654a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(file):\n",
    "    #file = file[0:17]\n",
    "    file = path+file\n",
    "    #print(file)\n",
    "    #print(path)\n",
    "    audio_signal, sample_rate = librosa.load(file)\n",
    "    mfcc_features = librosa.feature.mfcc(y = audio_signal, sr = sample_rate,n_mfcc = 40)\n",
    "    #print(mfcc_features)\n",
    "    mfcc_scaled_features = np.mean(mfcc_features.T, axis = 0)\n",
    "    \n",
    "    return mfcc_scaled_features\n",
    "\n",
    "def class_name(file):\n",
    "    \n",
    "    file_name = os.path.splitext(file)\n",
    "    class_name = file_name[0][1:4]\n",
    "    \n",
    "    return class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dde16f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anann\\AppData\\Local\\Temp\\ipykernel_19060\\4023326092.py:2: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_signal, sample_rate = librosa.load(filename)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '[cel][cla]0001__1.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\librosa\\core\\audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\librosa\\core\\audio.py:209\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1215\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[1;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[0;32m   1218\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[1;31mLibsndfileError\u001b[0m: Error opening '[cel][cla]0001__1.wav': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[cel][cla]0001__1.wav\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m audio_signal, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\librosa\\core\\audio.py:184\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[0;32m    181\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\librosa\\util\\decorators.py:60\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[0;32m     52\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[0;32m     59\u001b[0m )\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\librosa\\core\\audio.py:241\u001b[0m, in \u001b[0;36m__audioread_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    238\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[0;32m    244\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\audioread\\__init__.py:127\u001b[0m, in \u001b[0;36maudio_open\u001b[1;34m(path, backends)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m BackendClass \u001b[38;5;129;01min\u001b[39;00m backends:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBackendClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError:\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\audioread\\rawread.py:59\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m aifc\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '[cel][cla]0001__1.wav'"
     ]
    }
   ],
   "source": [
    "filename = '[cel][cla]0001__1.wav'\n",
    "audio_signal, sample_rate = librosa.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1acf9bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6706/6706 [03:09<00:00, 35.36it/s]\n"
     ]
    }
   ],
   "source": [
    "#EXTRACTING MFCC VALUES FOR EACH AND EVERY AUDIO FILE IN THE WHOLE DATASET\n",
    "\n",
    "from tqdm import tqdm\n",
    "path = 'D:\\Clg_Projects\\Music Dataset\\\\'\n",
    "audio_files = os.listdir(path)\n",
    "#print(audio_files)\n",
    "extracted_features = []\n",
    "for item in tqdm(audio_files):\n",
    "    if item.endswith('.wav'):\n",
    "        data = feature_extraction(item)\n",
    "        class_labels = class_name(item)\n",
    "        extracted_features.append([data,class_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2264d386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-405.65878, 135.69807, -28.65655, 8.057272, 1...</td>\n",
       "      <td>cel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-308.00064, 150.73721, -12.765315, 20.220102,...</td>\n",
       "      <td>cel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-401.90668, 143.44278, -5.4553065, 24.519606,...</td>\n",
       "      <td>cel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-402.59015, 122.679085, -22.919243, 31.608364...</td>\n",
       "      <td>cel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-387.77686, 128.7044, -14.35541, 20.25297, 1....</td>\n",
       "      <td>cel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature class\n",
       "0  [-405.65878, 135.69807, -28.65655, 8.057272, 1...   cel\n",
       "1  [-308.00064, 150.73721, -12.765315, 20.220102,...   cel\n",
       "2  [-401.90668, 143.44278, -5.4553065, 24.519606,...   cel\n",
       "3  [-402.59015, 122.679085, -22.919243, 31.608364...   cel\n",
       "4  [-387.77686, 128.7044, -14.35541, 20.25297, 1....   cel"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CONVERTING THE EXTRACTED MFCC FEATURES INTO A DATAFRAME WITH THEIR CORRESPONDING CLASS LABELS\n",
    "extracted_features_dataframe = pd.DataFrame(extracted_features,columns=['feature','class'])\n",
    "extracted_features_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0da90c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6700</th>\n",
       "      <td>[-56.473892, 93.0696, -5.95356, 30.59589, 3.01...</td>\n",
       "      <td>voi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6701</th>\n",
       "      <td>[-52.635315, 82.0335, -6.5638366, 31.444872, -...</td>\n",
       "      <td>voi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6702</th>\n",
       "      <td>[-93.49396, 80.49453, 0.70277464, 11.9647875, ...</td>\n",
       "      <td>voi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6703</th>\n",
       "      <td>[-85.9429, 62.03944, -9.492248, 22.346514, -9....</td>\n",
       "      <td>voi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6704</th>\n",
       "      <td>[-57.671223, 83.67719, -23.061024, 24.121489, ...</td>\n",
       "      <td>voi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                feature class\n",
       "6700  [-56.473892, 93.0696, -5.95356, 30.59589, 3.01...   voi\n",
       "6701  [-52.635315, 82.0335, -6.5638366, 31.444872, -...   voi\n",
       "6702  [-93.49396, 80.49453, 0.70277464, 11.9647875, ...   voi\n",
       "6703  [-85.9429, 62.03944, -9.492248, 22.346514, -9....   voi\n",
       "6704  [-57.671223, 83.67719, -23.061024, 24.121489, ...   voi"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features_dataframe.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a336e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITTING THE DATAFRAME INTO DEPENDENT AND INDEPENDENT VARIBALES AND CONVERTING THEM INTO AN ARRAY\n",
    "x = np.array(extracted_features_dataframe['feature'].tolist())\n",
    "y = np.array(extracted_features_dataframe['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6e0e53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6705, 40)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45350618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CONVERTING THE ARRAY 'y'(CATEGORICAL VARIABLE) INTO DUMMY VARIABLES\n",
    "y = np.array(pd.get_dummies(y))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c68af84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6705, 11)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0787486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITTING THE DATASET INTO TRAINING AND TESTING SET\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b060537a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.12.0-cp310-cp310-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.12.0\n",
      "  Downloading tensorflow_intel-2.12.0-cp310-cp310-win_amd64.whl (272.8 MB)\n",
      "     -------------------------------------- 272.8/272.8 MB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\anann\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (22.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\anann\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (65.6.3)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\anann\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.7.0)\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0\n",
      "  Using cached tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting tensorboard<2.13,>=2.12\n",
      "  Using cached tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.23.3-cp310-abi3-win_amd64.whl (422 kB)\n",
      "     -------------------------------------- 422.5/422.5 kB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\anann\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\anann\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
      "Collecting keras<2.13,>=2.12.0\n",
      "  Using cached keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.56.0-cp310-cp310-win_amd64.whl (4.2 MB)\n",
      "     ---------------------------------------- 4.2/4.2 MB 1.4 MB/s eta 0:00:00\n",
      "Collecting flatbuffers>=2.0\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-16.0.0-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "Collecting jax>=0.3.15\n",
      "  Downloading jax-0.4.13.tar.gz (1.3 MB)\n",
      "     ---------------------------------------- 1.3/1.3 MB 2.0 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\anann\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.4.0)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 2.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\anann\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\anann\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\anann\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.10.0)\n",
      "Collecting ml-dtypes>=0.1.0\n",
      "  Downloading ml_dtypes-0.2.0-cp310-cp310-win_amd64.whl (938 kB)\n",
      "     -------------------------------------- 938.6/938.6 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.20.0-py2.py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.5/181.5 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\anann\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\anann\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\anann\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.28.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\anann\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\anann\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.14)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anann\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anann\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\anann\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\anann\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\anann\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Building wheels for collected packages: jax\n",
      "  Building wheel for jax (pyproject.toml): started\n",
      "  Building wheel for jax (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for jax: filename=jax-0.4.13-py3-none-any.whl size=1518835 sha256=a0a24cc247a7ef805f0471c85f06fef16023556ad92a8822d33a983a1b7d9e80\n",
      "  Stored in directory: c:\\users\\anann\\appdata\\local\\pip\\cache\\wheels\\4c\\a3\\e7\\ea156aff3754a8f833f1b0c9587dec0bcfc9c551c439c9dcc7\n",
      "Successfully built jax\n",
      "Installing collected packages: libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, opt-einsum, oauthlib, ml-dtypes, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, jax, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.20.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.56.0 jax-0.4.13 keras-2.12.0 libclang-16.0.0 ml-dtypes-0.2.0 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.23.3 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.12.3 tensorboard-data-server-0.7.1 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-intel-2.12.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85805d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "efb97e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b07f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of classes in y\n",
    "num_labels = y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0396336d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5364, 40)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b086abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEQUENTIAL MODEL (ATTEMPT 1)\n",
    "model = Sequential()\n",
    "\n",
    "#FIRST LAYER\n",
    "model.add(Dense(100,input_shape = (40,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#SECOND LAYER\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#THIRD LAYER\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#FINAL LAYER\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "667be7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 100)               4100      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 200)               20200     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 200)               0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 11)                1111      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 11)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45,511\n",
      "Trainable params: 45,511\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7ad9e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9c580866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "138/168 [=======================>......] - ETA: 0s - loss: 1.5848 - accuracy: 0.4703\n",
      "Epoch 1: val_loss improved from inf to 1.64578, saving model to D:\\Clg_Projects\\music_instruments.hdf5\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 1.5776 - accuracy: 0.4718 - val_loss: 1.6458 - val_accuracy: 0.4825\n",
      "Epoch 2/250\n",
      "167/168 [============================>.] - ETA: 0s - loss: 1.6113 - accuracy: 0.4577\n",
      "Epoch 2: val_loss did not improve from 1.64578\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.6098 - accuracy: 0.4577 - val_loss: 1.6766 - val_accuracy: 0.4676\n",
      "Epoch 3/250\n",
      "166/168 [============================>.] - ETA: 0s - loss: 1.5846 - accuracy: 0.4710\n",
      "Epoch 3: val_loss did not improve from 1.64578\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5839 - accuracy: 0.4713 - val_loss: 1.6676 - val_accuracy: 0.4780\n",
      "Epoch 4/250\n",
      "152/168 [==========================>...] - ETA: 0s - loss: 1.5692 - accuracy: 0.4683\n",
      "Epoch 4: val_loss improved from 1.64578 to 1.64354, saving model to D:\\Clg_Projects\\music_instruments.hdf5\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5648 - accuracy: 0.4707 - val_loss: 1.6435 - val_accuracy: 0.4847\n",
      "Epoch 5/250\n",
      "152/168 [==========================>...] - ETA: 0s - loss: 1.6054 - accuracy: 0.4601\n",
      "Epoch 5: val_loss did not improve from 1.64354\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5998 - accuracy: 0.4635 - val_loss: 1.6531 - val_accuracy: 0.4758\n",
      "Epoch 6/250\n",
      "136/168 [=======================>......] - ETA: 0s - loss: 1.5867 - accuracy: 0.4619\n",
      "Epoch 6: val_loss did not improve from 1.64354\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5965 - accuracy: 0.4616 - val_loss: 1.6491 - val_accuracy: 0.4840\n",
      "Epoch 7/250\n",
      "140/168 [========================>.....] - ETA: 0s - loss: 1.5956 - accuracy: 0.4567\n",
      "Epoch 7: val_loss improved from 1.64354 to 1.64125, saving model to D:\\Clg_Projects\\music_instruments.hdf5\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5967 - accuracy: 0.4612 - val_loss: 1.6413 - val_accuracy: 0.4952\n",
      "Epoch 8/250\n",
      "161/168 [===========================>..] - ETA: 0s - loss: 1.5892 - accuracy: 0.4689\n",
      "Epoch 8: val_loss improved from 1.64125 to 1.63771, saving model to D:\\Clg_Projects\\music_instruments.hdf5\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 1.5875 - accuracy: 0.4702 - val_loss: 1.6377 - val_accuracy: 0.4937\n",
      "Epoch 9/250\n",
      "154/168 [==========================>...] - ETA: 0s - loss: 1.5875 - accuracy: 0.4690\n",
      "Epoch 9: val_loss did not improve from 1.63771\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5917 - accuracy: 0.4650 - val_loss: 1.6578 - val_accuracy: 0.4877\n",
      "Epoch 10/250\n",
      "140/168 [========================>.....] - ETA: 0s - loss: 1.6001 - accuracy: 0.4732\n",
      "Epoch 10: val_loss did not improve from 1.63771\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5817 - accuracy: 0.4776 - val_loss: 1.6595 - val_accuracy: 0.4922\n",
      "Epoch 11/250\n",
      "165/168 [============================>.] - ETA: 0s - loss: 1.5736 - accuracy: 0.4775\n",
      "Epoch 11: val_loss did not improve from 1.63771\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5746 - accuracy: 0.4778 - val_loss: 1.6679 - val_accuracy: 0.4817\n",
      "Epoch 12/250\n",
      "138/168 [=======================>......] - ETA: 0s - loss: 1.5970 - accuracy: 0.4660\n",
      "Epoch 12: val_loss did not improve from 1.63771\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5943 - accuracy: 0.4691 - val_loss: 1.6582 - val_accuracy: 0.4750\n",
      "Epoch 13/250\n",
      "168/168 [==============================] - ETA: 0s - loss: 1.5652 - accuracy: 0.4711\n",
      "Epoch 13: val_loss did not improve from 1.63771\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5652 - accuracy: 0.4711 - val_loss: 1.6379 - val_accuracy: 0.4981\n",
      "Epoch 14/250\n",
      "167/168 [============================>.] - ETA: 0s - loss: 1.5813 - accuracy: 0.4734\n",
      "Epoch 14: val_loss did not improve from 1.63771\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5823 - accuracy: 0.4730 - val_loss: 1.6507 - val_accuracy: 0.4870\n",
      "Epoch 15/250\n",
      "166/168 [============================>.] - ETA: 0s - loss: 1.5903 - accuracy: 0.4661\n",
      "Epoch 15: val_loss improved from 1.63771 to 1.63450, saving model to D:\\Clg_Projects\\music_instruments.hdf5\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.5920 - accuracy: 0.4655 - val_loss: 1.6345 - val_accuracy: 0.4795\n",
      "Epoch 16/250\n",
      "149/168 [=========================>....] - ETA: 0s - loss: 1.5915 - accuracy: 0.4669\n",
      "Epoch 16: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5958 - accuracy: 0.4664 - val_loss: 1.6574 - val_accuracy: 0.4750\n",
      "Epoch 17/250\n",
      "158/168 [===========================>..] - ETA: 0s - loss: 1.5968 - accuracy: 0.4612\n",
      "Epoch 17: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.5945 - accuracy: 0.4612 - val_loss: 1.6397 - val_accuracy: 0.4847\n",
      "Epoch 18/250\n",
      "155/168 [==========================>...] - ETA: 0s - loss: 1.6042 - accuracy: 0.4651\n",
      "Epoch 18: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.6067 - accuracy: 0.4648 - val_loss: 1.6458 - val_accuracy: 0.4750\n",
      "Epoch 19/250\n",
      "168/168 [==============================] - ETA: 0s - loss: 1.5864 - accuracy: 0.4653\n",
      "Epoch 19: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5864 - accuracy: 0.4653 - val_loss: 1.6708 - val_accuracy: 0.4720\n",
      "Epoch 20/250\n",
      "161/168 [===========================>..] - ETA: 0s - loss: 1.5998 - accuracy: 0.4587\n",
      "Epoch 20: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.6002 - accuracy: 0.4588 - val_loss: 1.6393 - val_accuracy: 0.4817\n",
      "Epoch 21/250\n",
      "152/168 [==========================>...] - ETA: 0s - loss: 1.6072 - accuracy: 0.4603\n",
      "Epoch 21: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.6028 - accuracy: 0.4627 - val_loss: 1.6477 - val_accuracy: 0.4795\n",
      "Epoch 22/250\n",
      "156/168 [==========================>...] - ETA: 0s - loss: 1.6011 - accuracy: 0.4615\n",
      "Epoch 22: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5988 - accuracy: 0.4629 - val_loss: 1.6349 - val_accuracy: 0.4773\n",
      "Epoch 23/250\n",
      "156/168 [==========================>...] - ETA: 0s - loss: 1.6141 - accuracy: 0.4601\n",
      "Epoch 23: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.6119 - accuracy: 0.4605 - val_loss: 1.6394 - val_accuracy: 0.4758\n",
      "Epoch 24/250\n",
      "144/168 [========================>.....] - ETA: 0s - loss: 1.5946 - accuracy: 0.4679\n",
      "Epoch 24: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5995 - accuracy: 0.4629 - val_loss: 1.6668 - val_accuracy: 0.4787\n",
      "Epoch 25/250\n",
      "150/168 [=========================>....] - ETA: 0s - loss: 1.6123 - accuracy: 0.4646\n",
      "Epoch 25: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.6062 - accuracy: 0.4664 - val_loss: 1.6604 - val_accuracy: 0.4832\n",
      "Epoch 26/250\n",
      "154/168 [==========================>...] - ETA: 0s - loss: 1.5769 - accuracy: 0.4738\n",
      "Epoch 26: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5804 - accuracy: 0.4718 - val_loss: 1.6413 - val_accuracy: 0.4981\n",
      "Epoch 27/250\n",
      "162/168 [===========================>..] - ETA: 0s - loss: 1.5869 - accuracy: 0.4703\n",
      "Epoch 27: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5894 - accuracy: 0.4691 - val_loss: 1.6531 - val_accuracy: 0.4728\n",
      "Epoch 28/250\n",
      "145/168 [========================>.....] - ETA: 0s - loss: 1.5761 - accuracy: 0.4707\n",
      "Epoch 28: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.6031 - accuracy: 0.4629 - val_loss: 1.6598 - val_accuracy: 0.4676\n",
      "Epoch 29/250\n",
      "142/168 [========================>.....] - ETA: 0s - loss: 1.5785 - accuracy: 0.4751\n",
      "Epoch 29: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5793 - accuracy: 0.4748 - val_loss: 1.6515 - val_accuracy: 0.4832\n",
      "Epoch 30/250\n",
      "160/168 [===========================>..] - ETA: 0s - loss: 1.5878 - accuracy: 0.4656\n",
      "Epoch 30: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5842 - accuracy: 0.4681 - val_loss: 1.6600 - val_accuracy: 0.4787\n",
      "Epoch 31/250\n",
      "165/168 [============================>.] - ETA: 0s - loss: 1.5844 - accuracy: 0.4695\n",
      "Epoch 31: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5833 - accuracy: 0.4711 - val_loss: 1.6485 - val_accuracy: 0.4877\n",
      "Epoch 32/250\n",
      "146/168 [=========================>....] - ETA: 0s - loss: 1.5631 - accuracy: 0.4803\n",
      "Epoch 32: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5707 - accuracy: 0.4750 - val_loss: 1.6561 - val_accuracy: 0.4817\n",
      "Epoch 33/250\n",
      "154/168 [==========================>...] - ETA: 0s - loss: 1.5958 - accuracy: 0.4690\n",
      "Epoch 33: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5962 - accuracy: 0.4689 - val_loss: 1.6456 - val_accuracy: 0.4758\n",
      "Epoch 34/250\n",
      "153/168 [==========================>...] - ETA: 0s - loss: 1.5901 - accuracy: 0.4737\n",
      "Epoch 34: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5796 - accuracy: 0.4754 - val_loss: 1.6515 - val_accuracy: 0.4825\n",
      "Epoch 35/250\n",
      "139/168 [=======================>......] - ETA: 0s - loss: 1.5905 - accuracy: 0.4656\n",
      "Epoch 35: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5905 - accuracy: 0.4674 - val_loss: 1.6539 - val_accuracy: 0.4743\n",
      "Epoch 36/250\n",
      "152/168 [==========================>...] - ETA: 0s - loss: 1.5884 - accuracy: 0.4688\n",
      "Epoch 36: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5879 - accuracy: 0.4683 - val_loss: 1.6643 - val_accuracy: 0.4787\n",
      "Epoch 37/250\n",
      "158/168 [===========================>..] - ETA: 0s - loss: 1.5693 - accuracy: 0.4721\n",
      "Epoch 37: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5712 - accuracy: 0.4726 - val_loss: 1.6634 - val_accuracy: 0.4773\n",
      "Epoch 38/250\n",
      "137/168 [=======================>......] - ETA: 0s - loss: 1.5881 - accuracy: 0.4740\n",
      "Epoch 38: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5892 - accuracy: 0.4735 - val_loss: 1.6571 - val_accuracy: 0.4743\n",
      "Epoch 39/250\n",
      "148/168 [=========================>....] - ETA: 0s - loss: 1.5839 - accuracy: 0.4747\n",
      "Epoch 39: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5740 - accuracy: 0.4774 - val_loss: 1.6435 - val_accuracy: 0.4862\n",
      "Epoch 40/250\n",
      "145/168 [========================>.....] - ETA: 0s - loss: 1.5844 - accuracy: 0.4651\n",
      "Epoch 40: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5809 - accuracy: 0.4657 - val_loss: 1.6398 - val_accuracy: 0.4758\n",
      "Epoch 41/250\n",
      "152/168 [==========================>...] - ETA: 0s - loss: 1.5737 - accuracy: 0.4671\n",
      "Epoch 41: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5798 - accuracy: 0.4650 - val_loss: 1.6662 - val_accuracy: 0.4758\n",
      "Epoch 42/250\n",
      "149/168 [=========================>....] - ETA: 0s - loss: 1.5739 - accuracy: 0.4797\n",
      "Epoch 42: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.5782 - accuracy: 0.4787 - val_loss: 1.6532 - val_accuracy: 0.4847\n",
      "Epoch 43/250\n",
      "153/168 [==========================>...] - ETA: 0s - loss: 1.5713 - accuracy: 0.4786\n",
      "Epoch 43: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5707 - accuracy: 0.4767 - val_loss: 1.6758 - val_accuracy: 0.4676\n",
      "Epoch 44/250\n",
      "159/168 [===========================>..] - ETA: 0s - loss: 1.5988 - accuracy: 0.4650\n",
      "Epoch 44: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 1.5957 - accuracy: 0.4668 - val_loss: 1.6615 - val_accuracy: 0.4787\n",
      "Epoch 45/250\n",
      "160/168 [===========================>..] - ETA: 0s - loss: 1.5863 - accuracy: 0.4674\n",
      "Epoch 45: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 1.5890 - accuracy: 0.4666 - val_loss: 1.6621 - val_accuracy: 0.4825\n",
      "Epoch 46/250\n",
      "148/168 [=========================>....] - ETA: 0s - loss: 1.5927 - accuracy: 0.4764\n",
      "Epoch 46: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5951 - accuracy: 0.4765 - val_loss: 1.6629 - val_accuracy: 0.4870\n",
      "Epoch 47/250\n",
      "154/168 [==========================>...] - ETA: 0s - loss: 1.5710 - accuracy: 0.4740\n",
      "Epoch 47: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5713 - accuracy: 0.4745 - val_loss: 1.6573 - val_accuracy: 0.4780\n",
      "Epoch 48/250\n",
      "158/168 [===========================>..] - ETA: 0s - loss: 1.5871 - accuracy: 0.4691\n",
      "Epoch 48: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.5865 - accuracy: 0.4705 - val_loss: 1.6645 - val_accuracy: 0.4810\n",
      "Epoch 49/250\n",
      "149/168 [=========================>....] - ETA: 0s - loss: 1.5806 - accuracy: 0.4713\n",
      "Epoch 49: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.5865 - accuracy: 0.4692 - val_loss: 1.6629 - val_accuracy: 0.4840\n",
      "Epoch 50/250\n",
      "155/168 [==========================>...] - ETA: 0s - loss: 1.5911 - accuracy: 0.4661\n",
      "Epoch 50: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5831 - accuracy: 0.4705 - val_loss: 1.6419 - val_accuracy: 0.4810\n",
      "Epoch 51/250\n",
      "165/168 [============================>.] - ETA: 0s - loss: 1.5937 - accuracy: 0.4644\n",
      "Epoch 51: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5983 - accuracy: 0.4633 - val_loss: 1.6351 - val_accuracy: 0.4966\n",
      "Epoch 52/250\n",
      "163/168 [============================>.] - ETA: 0s - loss: 1.5799 - accuracy: 0.4718\n",
      "Epoch 52: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5852 - accuracy: 0.4713 - val_loss: 1.6705 - val_accuracy: 0.4884\n",
      "Epoch 53/250\n",
      "152/168 [==========================>...] - ETA: 0s - loss: 1.5683 - accuracy: 0.4745\n",
      "Epoch 53: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5737 - accuracy: 0.4732 - val_loss: 1.6516 - val_accuracy: 0.4780\n",
      "Epoch 54/250\n",
      "161/168 [===========================>..] - ETA: 0s - loss: 1.5709 - accuracy: 0.4701\n",
      "Epoch 54: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5780 - accuracy: 0.4664 - val_loss: 1.6740 - val_accuracy: 0.4661\n",
      "Epoch 55/250\n",
      "154/168 [==========================>...] - ETA: 0s - loss: 1.5861 - accuracy: 0.4661\n",
      "Epoch 55: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5857 - accuracy: 0.4659 - val_loss: 1.6442 - val_accuracy: 0.4750\n",
      "Epoch 56/250\n",
      "161/168 [===========================>..] - ETA: 0s - loss: 1.5658 - accuracy: 0.4715\n",
      "Epoch 56: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5653 - accuracy: 0.4705 - val_loss: 1.6424 - val_accuracy: 0.4847\n",
      "Epoch 57/250\n",
      "147/168 [=========================>....] - ETA: 0s - loss: 1.5899 - accuracy: 0.4696\n",
      "Epoch 57: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.5944 - accuracy: 0.4691 - val_loss: 1.6602 - val_accuracy: 0.4787\n",
      "Epoch 58/250\n",
      "149/168 [=========================>....] - ETA: 0s - loss: 1.6063 - accuracy: 0.4673\n",
      "Epoch 58: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.6041 - accuracy: 0.4666 - val_loss: 1.6473 - val_accuracy: 0.4877\n",
      "Epoch 59/250\n",
      "166/168 [============================>.] - ETA: 0s - loss: 1.5919 - accuracy: 0.4674\n",
      "Epoch 59: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.5942 - accuracy: 0.4670 - val_loss: 1.6595 - val_accuracy: 0.4892\n",
      "Epoch 60/250\n",
      "166/168 [============================>.] - ETA: 0s - loss: 1.5865 - accuracy: 0.4695\n",
      "Epoch 60: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.5861 - accuracy: 0.4696 - val_loss: 1.6720 - val_accuracy: 0.4787\n",
      "Epoch 61/250\n",
      "167/168 [============================>.] - ETA: 0s - loss: 1.5797 - accuracy: 0.4725\n",
      "Epoch 61: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5817 - accuracy: 0.4718 - val_loss: 1.6406 - val_accuracy: 0.4929\n",
      "Epoch 62/250\n",
      "156/168 [==========================>...] - ETA: 0s - loss: 1.5768 - accuracy: 0.4796\n",
      "Epoch 62: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5723 - accuracy: 0.4804 - val_loss: 1.6469 - val_accuracy: 0.4765\n",
      "Epoch 63/250\n",
      "150/168 [=========================>....] - ETA: 0s - loss: 1.5910 - accuracy: 0.4654\n",
      "Epoch 63: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.5908 - accuracy: 0.4666 - val_loss: 1.6452 - val_accuracy: 0.4802\n",
      "Epoch 64/250\n",
      "167/168 [============================>.] - ETA: 0s - loss: 1.5799 - accuracy: 0.4719\n",
      "Epoch 64: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 1.5807 - accuracy: 0.4717 - val_loss: 1.6602 - val_accuracy: 0.4743\n",
      "Epoch 65/250\n",
      "142/168 [========================>.....] - ETA: 0s - loss: 1.5955 - accuracy: 0.4648\n",
      "Epoch 65: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.6004 - accuracy: 0.4653 - val_loss: 1.6530 - val_accuracy: 0.4832\n",
      "Epoch 66/250\n",
      "139/168 [=======================>......] - ETA: 0s - loss: 1.5976 - accuracy: 0.4712\n",
      "Epoch 66: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.6044 - accuracy: 0.4664 - val_loss: 1.6918 - val_accuracy: 0.4653\n",
      "Epoch 67/250\n",
      "164/168 [============================>.] - ETA: 0s - loss: 1.5786 - accuracy: 0.4800\n",
      "Epoch 67: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5788 - accuracy: 0.4799 - val_loss: 1.6775 - val_accuracy: 0.4795\n",
      "Epoch 68/250\n",
      "151/168 [=========================>....] - ETA: 0s - loss: 1.5949 - accuracy: 0.4609\n",
      "Epoch 68: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5866 - accuracy: 0.4623 - val_loss: 1.6540 - val_accuracy: 0.4728\n",
      "Epoch 69/250\n",
      "154/168 [==========================>...] - ETA: 0s - loss: 1.5981 - accuracy: 0.4637\n",
      "Epoch 69: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.6001 - accuracy: 0.4644 - val_loss: 1.6557 - val_accuracy: 0.4810\n",
      "Epoch 70/250\n",
      "165/168 [============================>.] - ETA: 0s - loss: 1.5919 - accuracy: 0.4636\n",
      "Epoch 70: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5893 - accuracy: 0.4651 - val_loss: 1.6815 - val_accuracy: 0.4750\n",
      "Epoch 71/250\n",
      "162/168 [===========================>..] - ETA: 0s - loss: 1.5634 - accuracy: 0.4705\n",
      "Epoch 71: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5640 - accuracy: 0.4694 - val_loss: 1.6587 - val_accuracy: 0.4870\n",
      "Epoch 72/250\n",
      "135/168 [=======================>......] - ETA: 0s - loss: 1.5911 - accuracy: 0.4650\n",
      "Epoch 72: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5981 - accuracy: 0.4646 - val_loss: 1.6519 - val_accuracy: 0.4899\n",
      "Epoch 73/250\n",
      "159/168 [===========================>..] - ETA: 0s - loss: 1.5932 - accuracy: 0.4697\n",
      "Epoch 73: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5889 - accuracy: 0.4704 - val_loss: 1.6513 - val_accuracy: 0.4780\n",
      "Epoch 74/250\n",
      "154/168 [==========================>...] - ETA: 0s - loss: 1.5825 - accuracy: 0.4716\n",
      "Epoch 74: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 1.5706 - accuracy: 0.4761 - val_loss: 1.6597 - val_accuracy: 0.4802\n",
      "Epoch 75/250\n",
      "156/168 [==========================>...] - ETA: 0s - loss: 1.5899 - accuracy: 0.4728\n",
      "Epoch 75: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 1.5834 - accuracy: 0.4728 - val_loss: 1.6618 - val_accuracy: 0.4810\n",
      "Epoch 76/250\n",
      "168/168 [==============================] - ETA: 0s - loss: 1.5793 - accuracy: 0.4705\n",
      "Epoch 76: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 1.5793 - accuracy: 0.4705 - val_loss: 1.6567 - val_accuracy: 0.4840\n",
      "Epoch 77/250\n",
      "168/168 [==============================] - ETA: 0s - loss: 1.5909 - accuracy: 0.4739\n",
      "Epoch 77: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 1.5909 - accuracy: 0.4739 - val_loss: 1.6365 - val_accuracy: 0.4832\n",
      "Epoch 78/250\n",
      "151/168 [=========================>....] - ETA: 0s - loss: 1.5683 - accuracy: 0.4776\n",
      "Epoch 78: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 1.5612 - accuracy: 0.4799 - val_loss: 1.6432 - val_accuracy: 0.4847\n",
      "Epoch 79/250\n",
      "166/168 [============================>.] - ETA: 0s - loss: 1.5576 - accuracy: 0.4797\n",
      "Epoch 79: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 1.5555 - accuracy: 0.4804 - val_loss: 1.6693 - val_accuracy: 0.4691\n",
      "Epoch 80/250\n",
      "153/168 [==========================>...] - ETA: 0s - loss: 1.5675 - accuracy: 0.4771\n",
      "Epoch 80: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5710 - accuracy: 0.4758 - val_loss: 1.6735 - val_accuracy: 0.4840\n",
      "Epoch 81/250\n",
      "161/168 [===========================>..] - ETA: 0s - loss: 1.5999 - accuracy: 0.4682\n",
      "Epoch 81: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 1.6038 - accuracy: 0.4689 - val_loss: 1.6491 - val_accuracy: 0.4832\n",
      "Epoch 82/250\n",
      "154/168 [==========================>...] - ETA: 0s - loss: 1.5827 - accuracy: 0.4759\n",
      "Epoch 82: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 1.5836 - accuracy: 0.4735 - val_loss: 1.6453 - val_accuracy: 0.4780\n",
      "Epoch 83/250\n",
      "139/168 [=======================>......] - ETA: 0s - loss: 1.5852 - accuracy: 0.4746\n",
      "Epoch 83: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5942 - accuracy: 0.4737 - val_loss: 1.6660 - val_accuracy: 0.4758\n",
      "Epoch 84/250\n",
      "156/168 [==========================>...] - ETA: 0s - loss: 1.5642 - accuracy: 0.4742\n",
      "Epoch 84: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.5663 - accuracy: 0.4765 - val_loss: 1.6431 - val_accuracy: 0.4884\n",
      "Epoch 85/250\n",
      "142/168 [========================>.....] - ETA: 0s - loss: 1.5738 - accuracy: 0.4778\n",
      "Epoch 85: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5828 - accuracy: 0.4741 - val_loss: 1.6365 - val_accuracy: 0.4914\n",
      "Epoch 86/250\n",
      "167/168 [============================>.] - ETA: 0s - loss: 1.5665 - accuracy: 0.4796\n",
      "Epoch 86: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.5676 - accuracy: 0.4795 - val_loss: 1.6421 - val_accuracy: 0.4855\n",
      "Epoch 87/250\n",
      "149/168 [=========================>....] - ETA: 0s - loss: 1.5484 - accuracy: 0.4813\n",
      "Epoch 87: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5524 - accuracy: 0.4806 - val_loss: 1.6517 - val_accuracy: 0.4743\n",
      "Epoch 88/250\n",
      "163/168 [============================>.] - ETA: 0s - loss: 1.6098 - accuracy: 0.4544\n",
      "Epoch 88: val_loss did not improve from 1.63450\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.6041 - accuracy: 0.4558 - val_loss: 1.6682 - val_accuracy: 0.4691\n",
      "Epoch 89/250\n",
      "158/168 [===========================>..] - ETA: 0s - loss: 1.5817 - accuracy: 0.4711\n",
      "Epoch 89: val_loss improved from 1.63450 to 1.62772, saving model to D:\\Clg_Projects\\music_instruments.hdf5\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.5802 - accuracy: 0.4724 - val_loss: 1.6277 - val_accuracy: 0.4907\n",
      "Epoch 90/250\n",
      "142/168 [========================>.....] - ETA: 0s - loss: 1.5690 - accuracy: 0.4716\n",
      "Epoch 90: val_loss improved from 1.62772 to 1.62453, saving model to D:\\Clg_Projects\\music_instruments.hdf5\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5698 - accuracy: 0.4750 - val_loss: 1.6245 - val_accuracy: 0.4884\n",
      "Epoch 91/250\n",
      "147/168 [=========================>....] - ETA: 0s - loss: 1.5786 - accuracy: 0.4813\n",
      "Epoch 91: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 1.5752 - accuracy: 0.4802 - val_loss: 1.6641 - val_accuracy: 0.4720\n",
      "Epoch 92/250\n",
      "159/168 [===========================>..] - ETA: 0s - loss: 1.5795 - accuracy: 0.4723\n",
      "Epoch 92: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.5710 - accuracy: 0.4739 - val_loss: 1.6573 - val_accuracy: 0.4847\n",
      "Epoch 93/250\n",
      "136/168 [=======================>......] - ETA: 0s - loss: 1.5822 - accuracy: 0.4724\n",
      "Epoch 93: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5811 - accuracy: 0.4704 - val_loss: 1.6700 - val_accuracy: 0.4765\n",
      "Epoch 94/250\n",
      "167/168 [============================>.] - ETA: 0s - loss: 1.5786 - accuracy: 0.4710\n",
      "Epoch 94: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5801 - accuracy: 0.4707 - val_loss: 1.6443 - val_accuracy: 0.4929\n",
      "Epoch 95/250\n",
      "153/168 [==========================>...] - ETA: 0s - loss: 1.5711 - accuracy: 0.4779\n",
      "Epoch 95: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5833 - accuracy: 0.4724 - val_loss: 1.6629 - val_accuracy: 0.4817\n",
      "Epoch 96/250\n",
      "139/168 [=======================>......] - ETA: 0s - loss: 1.5876 - accuracy: 0.4654\n",
      "Epoch 96: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5828 - accuracy: 0.4670 - val_loss: 1.6558 - val_accuracy: 0.4832\n",
      "Epoch 97/250\n",
      "135/168 [=======================>......] - ETA: 0s - loss: 1.5681 - accuracy: 0.4718\n",
      "Epoch 97: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5778 - accuracy: 0.4700 - val_loss: 1.6482 - val_accuracy: 0.4884\n",
      "Epoch 98/250\n",
      "162/168 [===========================>..] - ETA: 0s - loss: 1.5606 - accuracy: 0.4830\n",
      "Epoch 98: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.5631 - accuracy: 0.4815 - val_loss: 1.6406 - val_accuracy: 0.4832\n",
      "Epoch 99/250\n",
      "140/168 [========================>.....] - ETA: 0s - loss: 1.5702 - accuracy: 0.4710\n",
      "Epoch 99: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5641 - accuracy: 0.4698 - val_loss: 1.6520 - val_accuracy: 0.4810\n",
      "Epoch 100/250\n",
      "158/168 [===========================>..] - ETA: 0s - loss: 1.5818 - accuracy: 0.4747\n",
      "Epoch 100: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5793 - accuracy: 0.4773 - val_loss: 1.6763 - val_accuracy: 0.4817\n",
      "Epoch 101/250\n",
      "161/168 [===========================>..] - ETA: 0s - loss: 1.5811 - accuracy: 0.4678\n",
      "Epoch 101: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5783 - accuracy: 0.4696 - val_loss: 1.6549 - val_accuracy: 0.4870\n",
      "Epoch 102/250\n",
      "161/168 [===========================>..] - ETA: 0s - loss: 1.5701 - accuracy: 0.4740\n",
      "Epoch 102: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5747 - accuracy: 0.4730 - val_loss: 1.6481 - val_accuracy: 0.4884\n",
      "Epoch 103/250\n",
      "154/168 [==========================>...] - ETA: 0s - loss: 1.5819 - accuracy: 0.4653\n",
      "Epoch 103: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5742 - accuracy: 0.4666 - val_loss: 1.6311 - val_accuracy: 0.4877\n",
      "Epoch 104/250\n",
      "167/168 [============================>.] - ETA: 0s - loss: 1.5869 - accuracy: 0.4652\n",
      "Epoch 104: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5871 - accuracy: 0.4650 - val_loss: 1.6480 - val_accuracy: 0.4817\n",
      "Epoch 105/250\n",
      "150/168 [=========================>....] - ETA: 0s - loss: 1.5625 - accuracy: 0.4798\n",
      "Epoch 105: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5591 - accuracy: 0.4810 - val_loss: 1.6510 - val_accuracy: 0.4855\n",
      "Epoch 106/250\n",
      "165/168 [============================>.] - ETA: 0s - loss: 1.5809 - accuracy: 0.4759\n",
      "Epoch 106: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.5797 - accuracy: 0.4767 - val_loss: 1.6532 - val_accuracy: 0.4907\n",
      "Epoch 107/250\n",
      "160/168 [===========================>..] - ETA: 0s - loss: 1.5773 - accuracy: 0.4793\n",
      "Epoch 107: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5841 - accuracy: 0.4750 - val_loss: 1.6510 - val_accuracy: 0.4780\n",
      "Epoch 108/250\n",
      "161/168 [===========================>..] - ETA: 0s - loss: 1.5537 - accuracy: 0.4825\n",
      "Epoch 108: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5545 - accuracy: 0.4828 - val_loss: 1.6475 - val_accuracy: 0.4810\n",
      "Epoch 109/250\n",
      "151/168 [=========================>....] - ETA: 0s - loss: 1.5826 - accuracy: 0.4652\n",
      "Epoch 109: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5903 - accuracy: 0.4646 - val_loss: 1.6686 - val_accuracy: 0.4773\n",
      "Epoch 110/250\n",
      "141/168 [========================>.....] - ETA: 0s - loss: 1.5995 - accuracy: 0.4688\n",
      "Epoch 110: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5902 - accuracy: 0.4726 - val_loss: 1.6519 - val_accuracy: 0.4914\n",
      "Epoch 111/250\n",
      "141/168 [========================>.....] - ETA: 0s - loss: 1.5729 - accuracy: 0.4663\n",
      "Epoch 111: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5791 - accuracy: 0.4672 - val_loss: 1.6620 - val_accuracy: 0.4899\n",
      "Epoch 112/250\n",
      "149/168 [=========================>....] - ETA: 0s - loss: 1.5955 - accuracy: 0.4654\n",
      "Epoch 112: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5967 - accuracy: 0.4651 - val_loss: 1.6644 - val_accuracy: 0.4758\n",
      "Epoch 113/250\n",
      "163/168 [============================>.] - ETA: 0s - loss: 1.5918 - accuracy: 0.4699\n",
      "Epoch 113: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5907 - accuracy: 0.4711 - val_loss: 1.6781 - val_accuracy: 0.4683\n",
      "Epoch 114/250\n",
      "160/168 [===========================>..] - ETA: 0s - loss: 1.6003 - accuracy: 0.4633\n",
      "Epoch 114: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5980 - accuracy: 0.4650 - val_loss: 1.6487 - val_accuracy: 0.4802\n",
      "Epoch 115/250\n",
      "139/168 [=======================>......] - ETA: 0s - loss: 1.5787 - accuracy: 0.4784\n",
      "Epoch 115: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5867 - accuracy: 0.4763 - val_loss: 1.6507 - val_accuracy: 0.4847\n",
      "Epoch 116/250\n",
      "156/168 [==========================>...] - ETA: 0s - loss: 1.5813 - accuracy: 0.4706\n",
      "Epoch 116: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5825 - accuracy: 0.4698 - val_loss: 1.6807 - val_accuracy: 0.4705\n",
      "Epoch 117/250\n",
      "148/168 [=========================>....] - ETA: 0s - loss: 1.5931 - accuracy: 0.4681\n",
      "Epoch 117: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5827 - accuracy: 0.4717 - val_loss: 1.6724 - val_accuracy: 0.4661\n",
      "Epoch 118/250\n",
      "159/168 [===========================>..] - ETA: 0s - loss: 1.5855 - accuracy: 0.4707\n",
      "Epoch 118: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5874 - accuracy: 0.4713 - val_loss: 1.6493 - val_accuracy: 0.4832\n",
      "Epoch 119/250\n",
      "141/168 [========================>.....] - ETA: 0s - loss: 1.5453 - accuracy: 0.4827\n",
      "Epoch 119: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5608 - accuracy: 0.4769 - val_loss: 1.6498 - val_accuracy: 0.4750\n",
      "Epoch 120/250\n",
      "152/168 [==========================>...] - ETA: 0s - loss: 1.5466 - accuracy: 0.4850\n",
      "Epoch 120: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5508 - accuracy: 0.4866 - val_loss: 1.6623 - val_accuracy: 0.4780\n",
      "Epoch 121/250\n",
      "139/168 [=======================>......] - ETA: 0s - loss: 1.5629 - accuracy: 0.4800\n",
      "Epoch 121: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5681 - accuracy: 0.4808 - val_loss: 1.6375 - val_accuracy: 0.4974\n",
      "Epoch 122/250\n",
      "149/168 [=========================>....] - ETA: 0s - loss: 1.5938 - accuracy: 0.4685\n",
      "Epoch 122: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5900 - accuracy: 0.4694 - val_loss: 1.6488 - val_accuracy: 0.4862\n",
      "Epoch 123/250\n",
      "158/168 [===========================>..] - ETA: 0s - loss: 1.5600 - accuracy: 0.4820\n",
      "Epoch 123: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5560 - accuracy: 0.4817 - val_loss: 1.6496 - val_accuracy: 0.4892\n",
      "Epoch 124/250\n",
      "150/168 [=========================>....] - ETA: 0s - loss: 1.5543 - accuracy: 0.4771\n",
      "Epoch 124: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5528 - accuracy: 0.4778 - val_loss: 1.6470 - val_accuracy: 0.4750\n",
      "Epoch 125/250\n",
      "150/168 [=========================>....] - ETA: 0s - loss: 1.5833 - accuracy: 0.4737\n",
      "Epoch 125: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5927 - accuracy: 0.4709 - val_loss: 1.6465 - val_accuracy: 0.4922\n",
      "Epoch 126/250\n",
      "151/168 [=========================>....] - ETA: 0s - loss: 1.5825 - accuracy: 0.4766\n",
      "Epoch 126: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5896 - accuracy: 0.4745 - val_loss: 1.6515 - val_accuracy: 0.4810\n",
      "Epoch 127/250\n",
      "140/168 [========================>.....] - ETA: 0s - loss: 1.5884 - accuracy: 0.4652\n",
      "Epoch 127: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5947 - accuracy: 0.4594 - val_loss: 1.6567 - val_accuracy: 0.4758\n",
      "Epoch 128/250\n",
      "147/168 [=========================>....] - ETA: 0s - loss: 1.5707 - accuracy: 0.4719\n",
      "Epoch 128: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5754 - accuracy: 0.4677 - val_loss: 1.6498 - val_accuracy: 0.4810\n",
      "Epoch 129/250\n",
      "145/168 [========================>.....] - ETA: 0s - loss: 1.5677 - accuracy: 0.4668\n",
      "Epoch 129: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5588 - accuracy: 0.4707 - val_loss: 1.6699 - val_accuracy: 0.4840\n",
      "Epoch 130/250\n",
      "147/168 [=========================>....] - ETA: 0s - loss: 1.5616 - accuracy: 0.4709\n",
      "Epoch 130: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 1.5638 - accuracy: 0.4722 - val_loss: 1.6581 - val_accuracy: 0.4847\n",
      "Epoch 131/250\n",
      "140/168 [========================>.....] - ETA: 0s - loss: 1.5567 - accuracy: 0.4761\n",
      "Epoch 131: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5597 - accuracy: 0.4767 - val_loss: 1.6570 - val_accuracy: 0.4683\n",
      "Epoch 132/250\n",
      "155/168 [==========================>...] - ETA: 0s - loss: 1.5576 - accuracy: 0.4798\n",
      "Epoch 132: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5619 - accuracy: 0.4793 - val_loss: 1.6653 - val_accuracy: 0.4832\n",
      "Epoch 133/250\n",
      "143/168 [========================>.....] - ETA: 0s - loss: 1.5926 - accuracy: 0.4755\n",
      "Epoch 133: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5878 - accuracy: 0.4756 - val_loss: 1.6620 - val_accuracy: 0.4691\n",
      "Epoch 134/250\n",
      "146/168 [=========================>....] - ETA: 0s - loss: 1.5679 - accuracy: 0.4773\n",
      "Epoch 134: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5692 - accuracy: 0.4771 - val_loss: 1.6697 - val_accuracy: 0.4631\n",
      "Epoch 135/250\n",
      "168/168 [==============================] - ETA: 0s - loss: 1.5588 - accuracy: 0.4776\n",
      "Epoch 135: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5588 - accuracy: 0.4776 - val_loss: 1.6558 - val_accuracy: 0.4847\n",
      "Epoch 136/250\n",
      "160/168 [===========================>..] - ETA: 0s - loss: 1.5659 - accuracy: 0.4828\n",
      "Epoch 136: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5671 - accuracy: 0.4821 - val_loss: 1.6370 - val_accuracy: 0.4825\n",
      "Epoch 137/250\n",
      "153/168 [==========================>...] - ETA: 0s - loss: 1.5622 - accuracy: 0.4822\n",
      "Epoch 137: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5667 - accuracy: 0.4795 - val_loss: 1.6459 - val_accuracy: 0.4959\n",
      "Epoch 138/250\n",
      "167/168 [============================>.] - ETA: 0s - loss: 1.5737 - accuracy: 0.4755\n",
      "Epoch 138: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5747 - accuracy: 0.4746 - val_loss: 1.6858 - val_accuracy: 0.4780\n",
      "Epoch 139/250\n",
      "162/168 [===========================>..] - ETA: 0s - loss: 1.5693 - accuracy: 0.4830\n",
      "Epoch 139: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5705 - accuracy: 0.4836 - val_loss: 1.6602 - val_accuracy: 0.4720\n",
      "Epoch 140/250\n",
      "149/168 [=========================>....] - ETA: 0s - loss: 1.5825 - accuracy: 0.4692\n",
      "Epoch 140: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5757 - accuracy: 0.4726 - val_loss: 1.6559 - val_accuracy: 0.4743\n",
      "Epoch 141/250\n",
      "154/168 [==========================>...] - ETA: 0s - loss: 1.5625 - accuracy: 0.4870\n",
      "Epoch 141: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 1.5612 - accuracy: 0.4860 - val_loss: 1.6545 - val_accuracy: 0.4787\n",
      "Epoch 142/250\n",
      "158/168 [===========================>..] - ETA: 0s - loss: 1.5714 - accuracy: 0.4765\n",
      "Epoch 142: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.5678 - accuracy: 0.4778 - val_loss: 1.6564 - val_accuracy: 0.4735\n",
      "Epoch 143/250\n",
      "151/168 [=========================>....] - ETA: 0s - loss: 1.5857 - accuracy: 0.4694\n",
      "Epoch 143: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5913 - accuracy: 0.4666 - val_loss: 1.6613 - val_accuracy: 0.4773\n",
      "Epoch 144/250\n",
      "167/168 [============================>.] - ETA: 0s - loss: 1.5887 - accuracy: 0.4693\n",
      "Epoch 144: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5873 - accuracy: 0.4694 - val_loss: 1.6596 - val_accuracy: 0.4787\n",
      "Epoch 145/250\n",
      "131/168 [======================>.......] - ETA: 0s - loss: 1.5619 - accuracy: 0.4778\n",
      "Epoch 145: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5540 - accuracy: 0.4814 - val_loss: 1.6426 - val_accuracy: 0.4944\n",
      "Epoch 146/250\n",
      "165/168 [============================>.] - ETA: 0s - loss: 1.5881 - accuracy: 0.4778\n",
      "Epoch 146: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5921 - accuracy: 0.4761 - val_loss: 1.6525 - val_accuracy: 0.4907\n",
      "Epoch 147/250\n",
      "162/168 [===========================>..] - ETA: 0s - loss: 1.5464 - accuracy: 0.4902\n",
      "Epoch 147: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5456 - accuracy: 0.4911 - val_loss: 1.6669 - val_accuracy: 0.4743\n",
      "Epoch 148/250\n",
      "137/168 [=======================>......] - ETA: 0s - loss: 1.5581 - accuracy: 0.4806\n",
      "Epoch 148: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5676 - accuracy: 0.4825 - val_loss: 1.6891 - val_accuracy: 0.4750\n",
      "Epoch 149/250\n",
      "139/168 [=======================>......] - ETA: 0s - loss: 1.5511 - accuracy: 0.4845\n",
      "Epoch 149: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5486 - accuracy: 0.4851 - val_loss: 1.6633 - val_accuracy: 0.4735\n",
      "Epoch 150/250\n",
      "138/168 [=======================>......] - ETA: 0s - loss: 1.5703 - accuracy: 0.4803\n",
      "Epoch 150: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5676 - accuracy: 0.4847 - val_loss: 1.6625 - val_accuracy: 0.4773\n",
      "Epoch 151/250\n",
      "149/168 [=========================>....] - ETA: 0s - loss: 1.5498 - accuracy: 0.4822\n",
      "Epoch 151: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5491 - accuracy: 0.4825 - val_loss: 1.6661 - val_accuracy: 0.4713\n",
      "Epoch 152/250\n",
      "145/168 [========================>.....] - ETA: 0s - loss: 1.5885 - accuracy: 0.4750\n",
      "Epoch 152: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5851 - accuracy: 0.4761 - val_loss: 1.6523 - val_accuracy: 0.4989\n",
      "Epoch 153/250\n",
      "147/168 [=========================>....] - ETA: 0s - loss: 1.5715 - accuracy: 0.4741\n",
      "Epoch 153: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5744 - accuracy: 0.4750 - val_loss: 1.6611 - val_accuracy: 0.4862\n",
      "Epoch 154/250\n",
      "147/168 [=========================>....] - ETA: 0s - loss: 1.5581 - accuracy: 0.4849\n",
      "Epoch 154: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5600 - accuracy: 0.4832 - val_loss: 1.6458 - val_accuracy: 0.4929\n",
      "Epoch 155/250\n",
      "145/168 [========================>.....] - ETA: 0s - loss: 1.5421 - accuracy: 0.4849\n",
      "Epoch 155: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5464 - accuracy: 0.4823 - val_loss: 1.6800 - val_accuracy: 0.4720\n",
      "Epoch 156/250\n",
      "141/168 [========================>.....] - ETA: 0s - loss: 1.5792 - accuracy: 0.4752\n",
      "Epoch 156: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5800 - accuracy: 0.4778 - val_loss: 1.6591 - val_accuracy: 0.4810\n",
      "Epoch 157/250\n",
      "144/168 [========================>.....] - ETA: 0s - loss: 1.5741 - accuracy: 0.4733\n",
      "Epoch 157: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5778 - accuracy: 0.4709 - val_loss: 1.6614 - val_accuracy: 0.4750\n",
      "Epoch 158/250\n",
      "150/168 [=========================>....] - ETA: 0s - loss: 1.5526 - accuracy: 0.4810\n",
      "Epoch 158: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5521 - accuracy: 0.4791 - val_loss: 1.6717 - val_accuracy: 0.4728\n",
      "Epoch 159/250\n",
      "150/168 [=========================>....] - ETA: 0s - loss: 1.5662 - accuracy: 0.4792\n",
      "Epoch 159: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5672 - accuracy: 0.4786 - val_loss: 1.6562 - val_accuracy: 0.4877\n",
      "Epoch 160/250\n",
      "142/168 [========================>.....] - ETA: 0s - loss: 1.6054 - accuracy: 0.4769\n",
      "Epoch 160: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5917 - accuracy: 0.4814 - val_loss: 1.6902 - val_accuracy: 0.4810\n",
      "Epoch 161/250\n",
      "149/168 [=========================>....] - ETA: 0s - loss: 1.5736 - accuracy: 0.4790\n",
      "Epoch 161: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5777 - accuracy: 0.4782 - val_loss: 1.6747 - val_accuracy: 0.4676\n",
      "Epoch 162/250\n",
      "149/168 [=========================>....] - ETA: 0s - loss: 1.5487 - accuracy: 0.4839\n",
      "Epoch 162: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5425 - accuracy: 0.4871 - val_loss: 1.6563 - val_accuracy: 0.4847\n",
      "Epoch 163/250\n",
      "141/168 [========================>.....] - ETA: 0s - loss: 1.5543 - accuracy: 0.4767\n",
      "Epoch 163: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5580 - accuracy: 0.4769 - val_loss: 1.6539 - val_accuracy: 0.4780\n",
      "Epoch 164/250\n",
      "145/168 [========================>.....] - ETA: 0s - loss: 1.5899 - accuracy: 0.4739\n",
      "Epoch 164: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5894 - accuracy: 0.4737 - val_loss: 1.6604 - val_accuracy: 0.4825\n",
      "Epoch 165/250\n",
      "145/168 [========================>.....] - ETA: 0s - loss: 1.5476 - accuracy: 0.4804\n",
      "Epoch 165: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5484 - accuracy: 0.4795 - val_loss: 1.6469 - val_accuracy: 0.4825\n",
      "Epoch 166/250\n",
      "151/168 [=========================>....] - ETA: 0s - loss: 1.5633 - accuracy: 0.4785\n",
      "Epoch 166: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5737 - accuracy: 0.4760 - val_loss: 1.6745 - val_accuracy: 0.4668\n",
      "Epoch 167/250\n",
      "158/168 [===========================>..] - ETA: 0s - loss: 1.5831 - accuracy: 0.4693\n",
      "Epoch 167: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5815 - accuracy: 0.4709 - val_loss: 1.6704 - val_accuracy: 0.4743\n",
      "Epoch 168/250\n",
      "130/168 [======================>.......] - ETA: 0s - loss: 1.5755 - accuracy: 0.4707\n",
      "Epoch 168: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5791 - accuracy: 0.4711 - val_loss: 1.6821 - val_accuracy: 0.4750\n",
      "Epoch 169/250\n",
      "144/168 [========================>.....] - ETA: 0s - loss: 1.5303 - accuracy: 0.4831\n",
      "Epoch 169: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5378 - accuracy: 0.4802 - val_loss: 1.6523 - val_accuracy: 0.4795\n",
      "Epoch 170/250\n",
      "156/168 [==========================>...] - ETA: 0s - loss: 1.5597 - accuracy: 0.4808\n",
      "Epoch 170: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5665 - accuracy: 0.4773 - val_loss: 1.6716 - val_accuracy: 0.4795\n",
      "Epoch 171/250\n",
      "157/168 [===========================>..] - ETA: 0s - loss: 1.5538 - accuracy: 0.4829\n",
      "Epoch 171: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5573 - accuracy: 0.4815 - val_loss: 1.6605 - val_accuracy: 0.4884\n",
      "Epoch 172/250\n",
      "151/168 [=========================>....] - ETA: 0s - loss: 1.5550 - accuracy: 0.4801\n",
      "Epoch 172: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5549 - accuracy: 0.4797 - val_loss: 1.6666 - val_accuracy: 0.4765\n",
      "Epoch 173/250\n",
      "149/168 [=========================>....] - ETA: 0s - loss: 1.5564 - accuracy: 0.4813\n",
      "Epoch 173: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5489 - accuracy: 0.4851 - val_loss: 1.6676 - val_accuracy: 0.4795\n",
      "Epoch 174/250\n",
      "140/168 [========================>.....] - ETA: 0s - loss: 1.5641 - accuracy: 0.4792\n",
      "Epoch 174: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5779 - accuracy: 0.4748 - val_loss: 1.6807 - val_accuracy: 0.4787\n",
      "Epoch 175/250\n",
      "168/168 [==============================] - ETA: 0s - loss: 1.5590 - accuracy: 0.4810\n",
      "Epoch 175: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5590 - accuracy: 0.4810 - val_loss: 1.6726 - val_accuracy: 0.4780\n",
      "Epoch 176/250\n",
      "139/168 [=======================>......] - ETA: 0s - loss: 1.5554 - accuracy: 0.4768\n",
      "Epoch 176: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5519 - accuracy: 0.4812 - val_loss: 1.6614 - val_accuracy: 0.4795\n",
      "Epoch 177/250\n",
      "140/168 [========================>.....] - ETA: 0s - loss: 1.5578 - accuracy: 0.4710\n",
      "Epoch 177: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5784 - accuracy: 0.4677 - val_loss: 1.6611 - val_accuracy: 0.4758\n",
      "Epoch 178/250\n",
      "151/168 [=========================>....] - ETA: 0s - loss: 1.5666 - accuracy: 0.4779\n",
      "Epoch 178: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5682 - accuracy: 0.4787 - val_loss: 1.6604 - val_accuracy: 0.4884\n",
      "Epoch 179/250\n",
      "143/168 [========================>.....] - ETA: 0s - loss: 1.5795 - accuracy: 0.4744\n",
      "Epoch 179: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5861 - accuracy: 0.4732 - val_loss: 1.6472 - val_accuracy: 0.4825\n",
      "Epoch 180/250\n",
      "152/168 [==========================>...] - ETA: 0s - loss: 1.5735 - accuracy: 0.4749\n",
      "Epoch 180: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5723 - accuracy: 0.4741 - val_loss: 1.6497 - val_accuracy: 0.4787\n",
      "Epoch 181/250\n",
      "143/168 [========================>.....] - ETA: 0s - loss: 1.5839 - accuracy: 0.4747\n",
      "Epoch 181: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5850 - accuracy: 0.4709 - val_loss: 1.6458 - val_accuracy: 0.4832\n",
      "Epoch 182/250\n",
      "141/168 [========================>.....] - ETA: 0s - loss: 1.5854 - accuracy: 0.4725\n",
      "Epoch 182: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5726 - accuracy: 0.4752 - val_loss: 1.6258 - val_accuracy: 0.4892\n",
      "Epoch 183/250\n",
      "159/168 [===========================>..] - ETA: 0s - loss: 1.5708 - accuracy: 0.4752\n",
      "Epoch 183: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5654 - accuracy: 0.4758 - val_loss: 1.6277 - val_accuracy: 0.4817\n",
      "Epoch 184/250\n",
      "146/168 [=========================>....] - ETA: 0s - loss: 1.5565 - accuracy: 0.4827\n",
      "Epoch 184: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5563 - accuracy: 0.4860 - val_loss: 1.6501 - val_accuracy: 0.4750\n",
      "Epoch 185/250\n",
      "151/168 [=========================>....] - ETA: 0s - loss: 1.5592 - accuracy: 0.4888\n",
      "Epoch 185: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5594 - accuracy: 0.4892 - val_loss: 1.6605 - val_accuracy: 0.4847\n",
      "Epoch 186/250\n",
      "161/168 [===========================>..] - ETA: 0s - loss: 1.5414 - accuracy: 0.4837\n",
      "Epoch 186: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5415 - accuracy: 0.4828 - val_loss: 1.6531 - val_accuracy: 0.4877\n",
      "Epoch 187/250\n",
      "168/168 [==============================] - ETA: 0s - loss: 1.5480 - accuracy: 0.4789\n",
      "Epoch 187: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5480 - accuracy: 0.4789 - val_loss: 1.6285 - val_accuracy: 0.4929\n",
      "Epoch 188/250\n",
      "158/168 [===========================>..] - ETA: 0s - loss: 1.5421 - accuracy: 0.4907\n",
      "Epoch 188: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5536 - accuracy: 0.4868 - val_loss: 1.6588 - val_accuracy: 0.4892\n",
      "Epoch 189/250\n",
      "148/168 [=========================>....] - ETA: 0s - loss: 1.5792 - accuracy: 0.4810\n",
      "Epoch 189: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5767 - accuracy: 0.4842 - val_loss: 1.6640 - val_accuracy: 0.4840\n",
      "Epoch 190/250\n",
      "156/168 [==========================>...] - ETA: 0s - loss: 1.5614 - accuracy: 0.4718\n",
      "Epoch 190: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5590 - accuracy: 0.4750 - val_loss: 1.6524 - val_accuracy: 0.4817\n",
      "Epoch 191/250\n",
      "168/168 [==============================] - ETA: 0s - loss: 1.5600 - accuracy: 0.4756\n",
      "Epoch 191: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.5600 - accuracy: 0.4756 - val_loss: 1.6705 - val_accuracy: 0.4758\n",
      "Epoch 192/250\n",
      "161/168 [===========================>..] - ETA: 0s - loss: 1.5353 - accuracy: 0.4854\n",
      "Epoch 192: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5406 - accuracy: 0.4836 - val_loss: 1.6420 - val_accuracy: 0.4870\n",
      "Epoch 193/250\n",
      "160/168 [===========================>..] - ETA: 0s - loss: 1.5628 - accuracy: 0.4844\n",
      "Epoch 193: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5647 - accuracy: 0.4847 - val_loss: 1.6519 - val_accuracy: 0.4855\n",
      "Epoch 194/250\n",
      "135/168 [=======================>......] - ETA: 0s - loss: 1.5487 - accuracy: 0.4840\n",
      "Epoch 194: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5454 - accuracy: 0.4845 - val_loss: 1.6588 - val_accuracy: 0.4892\n",
      "Epoch 195/250\n",
      "155/168 [==========================>...] - ETA: 0s - loss: 1.5664 - accuracy: 0.4758\n",
      "Epoch 195: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5640 - accuracy: 0.4765 - val_loss: 1.6637 - val_accuracy: 0.4720\n",
      "Epoch 196/250\n",
      "162/168 [===========================>..] - ETA: 0s - loss: 1.5655 - accuracy: 0.4740\n",
      "Epoch 196: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5653 - accuracy: 0.4756 - val_loss: 1.6431 - val_accuracy: 0.4810\n",
      "Epoch 197/250\n",
      "137/168 [=======================>......] - ETA: 0s - loss: 1.5669 - accuracy: 0.4815\n",
      "Epoch 197: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5731 - accuracy: 0.4804 - val_loss: 1.6678 - val_accuracy: 0.4810\n",
      "Epoch 198/250\n",
      "162/168 [===========================>..] - ETA: 0s - loss: 1.5646 - accuracy: 0.4774\n",
      "Epoch 198: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5657 - accuracy: 0.4778 - val_loss: 1.6709 - val_accuracy: 0.4750\n",
      "Epoch 199/250\n",
      "166/168 [============================>.] - ETA: 0s - loss: 1.5445 - accuracy: 0.4872\n",
      "Epoch 199: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5419 - accuracy: 0.4890 - val_loss: 1.6590 - val_accuracy: 0.4728\n",
      "Epoch 200/250\n",
      "142/168 [========================>.....] - ETA: 0s - loss: 1.5624 - accuracy: 0.4826\n",
      "Epoch 200: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5520 - accuracy: 0.4834 - val_loss: 1.6469 - val_accuracy: 0.4787\n",
      "Epoch 201/250\n",
      "145/168 [========================>.....] - ETA: 0s - loss: 1.5796 - accuracy: 0.4677\n",
      "Epoch 201: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5829 - accuracy: 0.4683 - val_loss: 1.6537 - val_accuracy: 0.4713\n",
      "Epoch 202/250\n",
      "159/168 [===========================>..] - ETA: 0s - loss: 1.5645 - accuracy: 0.4744\n",
      "Epoch 202: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5748 - accuracy: 0.4732 - val_loss: 1.6356 - val_accuracy: 0.4840\n",
      "Epoch 203/250\n",
      "134/168 [======================>.......] - ETA: 0s - loss: 1.5464 - accuracy: 0.4879\n",
      "Epoch 203: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5552 - accuracy: 0.4838 - val_loss: 1.6611 - val_accuracy: 0.4877\n",
      "Epoch 204/250\n",
      "140/168 [========================>.....] - ETA: 0s - loss: 1.5590 - accuracy: 0.4792\n",
      "Epoch 204: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5619 - accuracy: 0.4802 - val_loss: 1.6470 - val_accuracy: 0.4750\n",
      "Epoch 205/250\n",
      "142/168 [========================>.....] - ETA: 0s - loss: 1.5552 - accuracy: 0.4831\n",
      "Epoch 205: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5473 - accuracy: 0.4862 - val_loss: 1.6634 - val_accuracy: 0.4855\n",
      "Epoch 206/250\n",
      "140/168 [========================>.....] - ETA: 0s - loss: 1.5705 - accuracy: 0.4790\n",
      "Epoch 206: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5741 - accuracy: 0.4746 - val_loss: 1.6630 - val_accuracy: 0.4802\n",
      "Epoch 207/250\n",
      "154/168 [==========================>...] - ETA: 0s - loss: 1.5505 - accuracy: 0.4823\n",
      "Epoch 207: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5508 - accuracy: 0.4832 - val_loss: 1.6586 - val_accuracy: 0.4758\n",
      "Epoch 208/250\n",
      "136/168 [=======================>......] - ETA: 0s - loss: 1.5506 - accuracy: 0.4830\n",
      "Epoch 208: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5612 - accuracy: 0.4789 - val_loss: 1.6593 - val_accuracy: 0.4817\n",
      "Epoch 209/250\n",
      "150/168 [=========================>....] - ETA: 0s - loss: 1.5653 - accuracy: 0.4760\n",
      "Epoch 209: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5694 - accuracy: 0.4735 - val_loss: 1.6875 - val_accuracy: 0.4735\n",
      "Epoch 210/250\n",
      "167/168 [============================>.] - ETA: 0s - loss: 1.5523 - accuracy: 0.4820\n",
      "Epoch 210: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5528 - accuracy: 0.4817 - val_loss: 1.6549 - val_accuracy: 0.4877\n",
      "Epoch 211/250\n",
      "137/168 [=======================>......] - ETA: 0s - loss: 1.5745 - accuracy: 0.4822\n",
      "Epoch 211: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5656 - accuracy: 0.4843 - val_loss: 1.6394 - val_accuracy: 0.4825\n",
      "Epoch 212/250\n",
      "147/168 [=========================>....] - ETA: 0s - loss: 1.5297 - accuracy: 0.4838\n",
      "Epoch 212: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5204 - accuracy: 0.4870 - val_loss: 1.6565 - val_accuracy: 0.4825\n",
      "Epoch 213/250\n",
      "141/168 [========================>.....] - ETA: 0s - loss: 1.5323 - accuracy: 0.4951\n",
      "Epoch 213: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5334 - accuracy: 0.4955 - val_loss: 1.6639 - val_accuracy: 0.4773\n",
      "Epoch 214/250\n",
      "137/168 [=======================>......] - ETA: 0s - loss: 1.5304 - accuracy: 0.4934\n",
      "Epoch 214: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5378 - accuracy: 0.4901 - val_loss: 1.6561 - val_accuracy: 0.4765\n",
      "Epoch 215/250\n",
      "142/168 [========================>.....] - ETA: 0s - loss: 1.5459 - accuracy: 0.4842\n",
      "Epoch 215: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5545 - accuracy: 0.4819 - val_loss: 1.6668 - val_accuracy: 0.4810\n",
      "Epoch 216/250\n",
      "135/168 [=======================>......] - ETA: 0s - loss: 1.5275 - accuracy: 0.4981\n",
      "Epoch 216: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5464 - accuracy: 0.4896 - val_loss: 1.6786 - val_accuracy: 0.4780\n",
      "Epoch 217/250\n",
      "150/168 [=========================>....] - ETA: 0s - loss: 1.5539 - accuracy: 0.4821\n",
      "Epoch 217: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5534 - accuracy: 0.4814 - val_loss: 1.6772 - val_accuracy: 0.4728\n",
      "Epoch 218/250\n",
      "142/168 [========================>.....] - ETA: 0s - loss: 1.5494 - accuracy: 0.4875\n",
      "Epoch 218: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5510 - accuracy: 0.4828 - val_loss: 1.6539 - val_accuracy: 0.4832\n",
      "Epoch 219/250\n",
      "166/168 [============================>.] - ETA: 0s - loss: 1.5250 - accuracy: 0.4866\n",
      "Epoch 219: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5234 - accuracy: 0.4868 - val_loss: 1.6642 - val_accuracy: 0.4750\n",
      "Epoch 220/250\n",
      "145/168 [========================>.....] - ETA: 0s - loss: 1.5603 - accuracy: 0.4853\n",
      "Epoch 220: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5592 - accuracy: 0.4860 - val_loss: 1.6978 - val_accuracy: 0.4653\n",
      "Epoch 221/250\n",
      "140/168 [========================>.....] - ETA: 0s - loss: 1.5569 - accuracy: 0.4783\n",
      "Epoch 221: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5542 - accuracy: 0.4765 - val_loss: 1.6636 - val_accuracy: 0.4847\n",
      "Epoch 222/250\n",
      "143/168 [========================>.....] - ETA: 0s - loss: 1.5487 - accuracy: 0.4862\n",
      "Epoch 222: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5511 - accuracy: 0.4877 - val_loss: 1.6527 - val_accuracy: 0.4825\n",
      "Epoch 223/250\n",
      "140/168 [========================>.....] - ETA: 0s - loss: 1.5766 - accuracy: 0.4799\n",
      "Epoch 223: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5689 - accuracy: 0.4828 - val_loss: 1.6617 - val_accuracy: 0.4832\n",
      "Epoch 224/250\n",
      "147/168 [=========================>....] - ETA: 0s - loss: 1.5451 - accuracy: 0.4923\n",
      "Epoch 224: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5496 - accuracy: 0.4909 - val_loss: 1.6542 - val_accuracy: 0.4937\n",
      "Epoch 225/250\n",
      "141/168 [========================>.....] - ETA: 0s - loss: 1.5376 - accuracy: 0.4885\n",
      "Epoch 225: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5375 - accuracy: 0.4884 - val_loss: 1.6550 - val_accuracy: 0.4855\n",
      "Epoch 226/250\n",
      "168/168 [==============================] - ETA: 0s - loss: 1.5625 - accuracy: 0.4761\n",
      "Epoch 226: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5625 - accuracy: 0.4761 - val_loss: 1.6789 - val_accuracy: 0.4720\n",
      "Epoch 227/250\n",
      "146/168 [=========================>....] - ETA: 0s - loss: 1.5623 - accuracy: 0.4739\n",
      "Epoch 227: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5530 - accuracy: 0.4760 - val_loss: 1.6874 - val_accuracy: 0.4713\n",
      "Epoch 228/250\n",
      "141/168 [========================>.....] - ETA: 0s - loss: 1.5456 - accuracy: 0.4854\n",
      "Epoch 228: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5578 - accuracy: 0.4864 - val_loss: 1.6535 - val_accuracy: 0.4870\n",
      "Epoch 229/250\n",
      "139/168 [=======================>......] - ETA: 0s - loss: 1.5256 - accuracy: 0.4903\n",
      "Epoch 229: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5360 - accuracy: 0.4909 - val_loss: 1.6779 - val_accuracy: 0.4705\n",
      "Epoch 230/250\n",
      "144/168 [========================>.....] - ETA: 0s - loss: 1.5448 - accuracy: 0.4846\n",
      "Epoch 230: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5454 - accuracy: 0.4834 - val_loss: 1.6700 - val_accuracy: 0.4862\n",
      "Epoch 231/250\n",
      "159/168 [===========================>..] - ETA: 0s - loss: 1.5458 - accuracy: 0.4849\n",
      "Epoch 231: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5422 - accuracy: 0.4851 - val_loss: 1.6559 - val_accuracy: 0.4825\n",
      "Epoch 232/250\n",
      "148/168 [=========================>....] - ETA: 0s - loss: 1.5581 - accuracy: 0.4831\n",
      "Epoch 232: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5624 - accuracy: 0.4819 - val_loss: 1.6577 - val_accuracy: 0.4780\n",
      "Epoch 233/250\n",
      "157/168 [===========================>..] - ETA: 0s - loss: 1.5548 - accuracy: 0.4819\n",
      "Epoch 233: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5500 - accuracy: 0.4845 - val_loss: 1.6576 - val_accuracy: 0.4832\n",
      "Epoch 234/250\n",
      "143/168 [========================>.....] - ETA: 0s - loss: 1.5540 - accuracy: 0.4801\n",
      "Epoch 234: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5539 - accuracy: 0.4815 - val_loss: 1.6611 - val_accuracy: 0.4892\n",
      "Epoch 235/250\n",
      "168/168 [==============================] - ETA: 0s - loss: 1.5707 - accuracy: 0.4804\n",
      "Epoch 235: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.5707 - accuracy: 0.4804 - val_loss: 1.6729 - val_accuracy: 0.4840\n",
      "Epoch 236/250\n",
      "154/168 [==========================>...] - ETA: 0s - loss: 1.5600 - accuracy: 0.4773\n",
      "Epoch 236: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 1.5531 - accuracy: 0.4782 - val_loss: 1.6708 - val_accuracy: 0.4720\n",
      "Epoch 237/250\n",
      "160/168 [===========================>..] - ETA: 0s - loss: 1.5452 - accuracy: 0.4863\n",
      "Epoch 237: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5428 - accuracy: 0.4879 - val_loss: 1.6522 - val_accuracy: 0.4884\n",
      "Epoch 238/250\n",
      "161/168 [===========================>..] - ETA: 0s - loss: 1.5776 - accuracy: 0.4839\n",
      "Epoch 238: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5787 - accuracy: 0.4845 - val_loss: 1.6814 - val_accuracy: 0.4802\n",
      "Epoch 239/250\n",
      "161/168 [===========================>..] - ETA: 0s - loss: 1.5692 - accuracy: 0.4792\n",
      "Epoch 239: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5666 - accuracy: 0.4801 - val_loss: 1.6460 - val_accuracy: 0.4937\n",
      "Epoch 240/250\n",
      "160/168 [===========================>..] - ETA: 0s - loss: 1.5447 - accuracy: 0.4836\n",
      "Epoch 240: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 1.5356 - accuracy: 0.4862 - val_loss: 1.6596 - val_accuracy: 0.4802\n",
      "Epoch 241/250\n",
      "142/168 [========================>.....] - ETA: 0s - loss: 1.5741 - accuracy: 0.4773\n",
      "Epoch 241: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5785 - accuracy: 0.4767 - val_loss: 1.7089 - val_accuracy: 0.4609\n",
      "Epoch 242/250\n",
      "165/168 [============================>.] - ETA: 0s - loss: 1.5624 - accuracy: 0.4786\n",
      "Epoch 242: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5580 - accuracy: 0.4804 - val_loss: 1.6683 - val_accuracy: 0.4817\n",
      "Epoch 243/250\n",
      "143/168 [========================>.....] - ETA: 0s - loss: 1.5354 - accuracy: 0.4889\n",
      "Epoch 243: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5431 - accuracy: 0.4871 - val_loss: 1.6637 - val_accuracy: 0.4750\n",
      "Epoch 244/250\n",
      "164/168 [============================>.] - ETA: 0s - loss: 1.5624 - accuracy: 0.4781\n",
      "Epoch 244: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5612 - accuracy: 0.4780 - val_loss: 1.6766 - val_accuracy: 0.4646\n",
      "Epoch 245/250\n",
      "139/168 [=======================>......] - ETA: 0s - loss: 1.5522 - accuracy: 0.4852\n",
      "Epoch 245: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5598 - accuracy: 0.4832 - val_loss: 1.6838 - val_accuracy: 0.4616\n",
      "Epoch 246/250\n",
      "149/168 [=========================>....] - ETA: 0s - loss: 1.5920 - accuracy: 0.4746\n",
      "Epoch 246: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5864 - accuracy: 0.4776 - val_loss: 1.6820 - val_accuracy: 0.4705\n",
      "Epoch 247/250\n",
      "139/168 [=======================>......] - ETA: 0s - loss: 1.5646 - accuracy: 0.4777\n",
      "Epoch 247: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5593 - accuracy: 0.4814 - val_loss: 1.6698 - val_accuracy: 0.4817\n",
      "Epoch 248/250\n",
      "148/168 [=========================>....] - ETA: 0s - loss: 1.5514 - accuracy: 0.4894\n",
      "Epoch 248: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5471 - accuracy: 0.4914 - val_loss: 1.6707 - val_accuracy: 0.4870\n",
      "Epoch 249/250\n",
      "140/168 [========================>.....] - ETA: 0s - loss: 1.5634 - accuracy: 0.4884\n",
      "Epoch 249: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5606 - accuracy: 0.4888 - val_loss: 1.6773 - val_accuracy: 0.4601\n",
      "Epoch 250/250\n",
      "167/168 [============================>.] - ETA: 0s - loss: 1.5721 - accuracy: 0.4770\n",
      "Epoch 250: val_loss did not improve from 1.62453\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5739 - accuracy: 0.4761 - val_loss: 1.6759 - val_accuracy: 0.4840\n",
      "Training completed in time:  0:01:43.531268\n"
     ]
    }
   ],
   "source": [
    "#TRAINING A MODEL(ATTEMPT 1)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "\n",
    "num_epochs = 250\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath = 'D:\\Clg_Projects\\music_instruments.hdf5',verbose = 1, save_best_only = True)\n",
    "\n",
    "start = datetime.now()\n",
    "model.fit(x_train,y_train,batch_size = num_batch_size,epochs = num_epochs, validation_data = (x_test,y_test),callbacks = [checkpointer])\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9411af72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4839671850204468"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy = model.evaluate(x_test,y_test,verbose = 0)\n",
    "test_accuracy[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d439f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL AND TRAINING (ATTEMPT 2)\n",
    "\n",
    "#Defining the neural network architecture\n",
    "#model = tf.keras.\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#FIRST LAYER\n",
    "model.add(Dense(100,input_shape = (40,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#SECOND LAYER\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#THIRD LAYER\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#FINAL LAYER\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "17fe2531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_13 (Dense)            (None, 100)               4100      \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               12928     \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 100)               12900     \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 11)                1111      \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 11)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,039\n",
      "Trainable params: 31,039\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f5997e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "49ce7ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "164/168 [============================>.] - ETA: 0s - loss: 1.6560 - accuracy: 0.4304\n",
      "Epoch 1: val_loss improved from inf to 1.74193, saving model to D:\\Clg_Projects\\music_instruments(attempt2).hdf5\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 1.6551 - accuracy: 0.4312 - val_loss: 1.7419 - val_accuracy: 0.4273\n",
      "Epoch 2/100\n",
      "166/168 [============================>.] - ETA: 0s - loss: 1.6478 - accuracy: 0.4337\n",
      "Epoch 2: val_loss improved from 1.74193 to 1.72286, saving model to D:\\Clg_Projects\\music_instruments(attempt2).hdf5\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.6482 - accuracy: 0.4338 - val_loss: 1.7229 - val_accuracy: 0.4295\n",
      "Epoch 3/100\n",
      "167/168 [============================>.] - ETA: 0s - loss: 1.6438 - accuracy: 0.4366\n",
      "Epoch 3: val_loss did not improve from 1.72286\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.6465 - accuracy: 0.4355 - val_loss: 1.7494 - val_accuracy: 0.4079\n",
      "Epoch 4/100\n",
      "144/168 [========================>.....] - ETA: 0s - loss: 1.6016 - accuracy: 0.4562\n",
      "Epoch 4: val_loss did not improve from 1.72286\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.6150 - accuracy: 0.4506 - val_loss: 1.7339 - val_accuracy: 0.4146\n",
      "Epoch 5/100\n",
      "151/168 [=========================>....] - ETA: 0s - loss: 1.6327 - accuracy: 0.4491\n",
      "Epoch 5: val_loss improved from 1.72286 to 1.69337, saving model to D:\\Clg_Projects\\music_instruments(attempt2).hdf5\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.6300 - accuracy: 0.4497 - val_loss: 1.6934 - val_accuracy: 0.4407\n",
      "Epoch 6/100\n",
      "164/168 [============================>.] - ETA: 0s - loss: 1.6330 - accuracy: 0.4451\n",
      "Epoch 6: val_loss did not improve from 1.69337\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.6326 - accuracy: 0.4457 - val_loss: 1.7242 - val_accuracy: 0.4437\n",
      "Epoch 7/100\n",
      "151/168 [=========================>....] - ETA: 0s - loss: 1.5953 - accuracy: 0.4570\n",
      "Epoch 7: val_loss did not improve from 1.69337\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.6012 - accuracy: 0.4590 - val_loss: 1.7391 - val_accuracy: 0.4392\n",
      "Epoch 8/100\n",
      "158/168 [===========================>..] - ETA: 0s - loss: 1.6159 - accuracy: 0.4537\n",
      "Epoch 8: val_loss did not improve from 1.69337\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.6203 - accuracy: 0.4502 - val_loss: 1.7271 - val_accuracy: 0.4280\n",
      "Epoch 9/100\n",
      "156/168 [==========================>...] - ETA: 0s - loss: 1.6187 - accuracy: 0.4541\n",
      "Epoch 9: val_loss did not improve from 1.69337\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.6204 - accuracy: 0.4536 - val_loss: 1.7093 - val_accuracy: 0.4407\n",
      "Epoch 10/100\n",
      "156/168 [==========================>...] - ETA: 0s - loss: 1.6095 - accuracy: 0.4539\n",
      "Epoch 10: val_loss did not improve from 1.69337\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.6062 - accuracy: 0.4556 - val_loss: 1.7035 - val_accuracy: 0.4482\n",
      "Epoch 11/100\n",
      "149/168 [=========================>....] - ETA: 0s - loss: 1.5858 - accuracy: 0.4599\n",
      "Epoch 11: val_loss did not improve from 1.69337\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5934 - accuracy: 0.4577 - val_loss: 1.7180 - val_accuracy: 0.4467\n",
      "Epoch 12/100\n",
      "143/168 [========================>.....] - ETA: 0s - loss: 1.5914 - accuracy: 0.4615\n",
      "Epoch 12: val_loss did not improve from 1.69337\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5902 - accuracy: 0.4603 - val_loss: 1.7216 - val_accuracy: 0.4422\n",
      "Epoch 13/100\n",
      "144/168 [========================>.....] - ETA: 0s - loss: 1.6058 - accuracy: 0.4590\n",
      "Epoch 13: val_loss did not improve from 1.69337\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.6031 - accuracy: 0.4597 - val_loss: 1.7603 - val_accuracy: 0.4340\n",
      "Epoch 14/100\n",
      "163/168 [============================>.] - ETA: 0s - loss: 1.6241 - accuracy: 0.4507\n",
      "Epoch 14: val_loss did not improve from 1.69337\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.6207 - accuracy: 0.4523 - val_loss: 1.7043 - val_accuracy: 0.4415\n",
      "Epoch 15/100\n",
      "136/168 [=======================>......] - ETA: 0s - loss: 1.6099 - accuracy: 0.4598\n",
      "Epoch 15: val_loss did not improve from 1.69337\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.6076 - accuracy: 0.4610 - val_loss: 1.6966 - val_accuracy: 0.4407\n",
      "Epoch 16/100\n",
      "151/168 [=========================>....] - ETA: 0s - loss: 1.5890 - accuracy: 0.4665\n",
      "Epoch 16: val_loss did not improve from 1.69337\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5868 - accuracy: 0.4666 - val_loss: 1.7054 - val_accuracy: 0.4415\n",
      "Epoch 17/100\n",
      "166/168 [============================>.] - ETA: 0s - loss: 1.5981 - accuracy: 0.4625\n",
      "Epoch 17: val_loss did not improve from 1.69337\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5940 - accuracy: 0.4638 - val_loss: 1.6977 - val_accuracy: 0.4444\n",
      "Epoch 18/100\n",
      "145/168 [========================>.....] - ETA: 0s - loss: 1.5969 - accuracy: 0.4720\n",
      "Epoch 18: val_loss did not improve from 1.69337\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.6031 - accuracy: 0.4702 - val_loss: 1.7286 - val_accuracy: 0.4265\n",
      "Epoch 19/100\n",
      "144/168 [========================>.....] - ETA: 0s - loss: 1.5852 - accuracy: 0.4674\n",
      "Epoch 19: val_loss improved from 1.69337 to 1.67917, saving model to D:\\Clg_Projects\\music_instruments(attempt2).hdf5\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5756 - accuracy: 0.4702 - val_loss: 1.6792 - val_accuracy: 0.4564\n",
      "Epoch 20/100\n",
      "156/168 [==========================>...] - ETA: 0s - loss: 1.5765 - accuracy: 0.4561\n",
      "Epoch 20: val_loss did not improve from 1.67917\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5783 - accuracy: 0.4586 - val_loss: 1.6919 - val_accuracy: 0.4467\n",
      "Epoch 21/100\n",
      "143/168 [========================>.....] - ETA: 0s - loss: 1.5878 - accuracy: 0.4633\n",
      "Epoch 21: val_loss improved from 1.67917 to 1.67036, saving model to D:\\Clg_Projects\\music_instruments(attempt2).hdf5\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5847 - accuracy: 0.4659 - val_loss: 1.6704 - val_accuracy: 0.4564\n",
      "Epoch 22/100\n",
      "159/168 [===========================>..] - ETA: 0s - loss: 1.5723 - accuracy: 0.4703\n",
      "Epoch 22: val_loss did not improve from 1.67036\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5758 - accuracy: 0.4691 - val_loss: 1.7089 - val_accuracy: 0.4497\n",
      "Epoch 23/100\n",
      "161/168 [===========================>..] - ETA: 0s - loss: 1.5725 - accuracy: 0.4695\n",
      "Epoch 23: val_loss improved from 1.67036 to 1.66255, saving model to D:\\Clg_Projects\\music_instruments(attempt2).hdf5\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.5712 - accuracy: 0.4713 - val_loss: 1.6625 - val_accuracy: 0.4683\n",
      "Epoch 24/100\n",
      "145/168 [========================>.....] - ETA: 0s - loss: 1.5615 - accuracy: 0.4728\n",
      "Epoch 24: val_loss did not improve from 1.66255\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.5641 - accuracy: 0.4694 - val_loss: 1.7124 - val_accuracy: 0.4415\n",
      "Epoch 25/100\n",
      "148/168 [=========================>....] - ETA: 0s - loss: 1.5687 - accuracy: 0.4749\n",
      "Epoch 25: val_loss did not improve from 1.66255\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5677 - accuracy: 0.4780 - val_loss: 1.6653 - val_accuracy: 0.4594\n",
      "Epoch 26/100\n",
      "137/168 [=======================>......] - ETA: 0s - loss: 1.5847 - accuracy: 0.4751\n",
      "Epoch 26: val_loss did not improve from 1.66255\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5874 - accuracy: 0.4720 - val_loss: 1.7011 - val_accuracy: 0.4459\n",
      "Epoch 27/100\n",
      "141/168 [========================>.....] - ETA: 0s - loss: 1.5778 - accuracy: 0.4763\n",
      "Epoch 27: val_loss did not improve from 1.66255\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5767 - accuracy: 0.4787 - val_loss: 1.6953 - val_accuracy: 0.4601\n",
      "Epoch 28/100\n",
      "159/168 [===========================>..] - ETA: 0s - loss: 1.5470 - accuracy: 0.4741\n",
      "Epoch 28: val_loss did not improve from 1.66255\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5561 - accuracy: 0.4713 - val_loss: 1.6763 - val_accuracy: 0.4519\n",
      "Epoch 29/100\n",
      "167/168 [============================>.] - ETA: 0s - loss: 1.5549 - accuracy: 0.4830\n",
      "Epoch 29: val_loss did not improve from 1.66255\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5535 - accuracy: 0.4836 - val_loss: 1.6792 - val_accuracy: 0.4474\n",
      "Epoch 30/100\n",
      "141/168 [========================>.....] - ETA: 0s - loss: 1.5592 - accuracy: 0.4871\n",
      "Epoch 30: val_loss did not improve from 1.66255\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5544 - accuracy: 0.4832 - val_loss: 1.6824 - val_accuracy: 0.4459\n",
      "Epoch 31/100\n",
      "167/168 [============================>.] - ETA: 0s - loss: 1.5549 - accuracy: 0.4809\n",
      "Epoch 31: val_loss did not improve from 1.66255\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5551 - accuracy: 0.4808 - val_loss: 1.6787 - val_accuracy: 0.4571\n",
      "Epoch 32/100\n",
      "149/168 [=========================>....] - ETA: 0s - loss: 1.5527 - accuracy: 0.4824\n",
      "Epoch 32: val_loss did not improve from 1.66255\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5522 - accuracy: 0.4810 - val_loss: 1.6823 - val_accuracy: 0.4571\n",
      "Epoch 33/100\n",
      "160/168 [===========================>..] - ETA: 0s - loss: 1.5734 - accuracy: 0.4711\n",
      "Epoch 33: val_loss did not improve from 1.66255\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5740 - accuracy: 0.4705 - val_loss: 1.7064 - val_accuracy: 0.4489\n",
      "Epoch 34/100\n",
      "146/168 [=========================>....] - ETA: 0s - loss: 1.5186 - accuracy: 0.4863\n",
      "Epoch 34: val_loss did not improve from 1.66255\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5281 - accuracy: 0.4830 - val_loss: 1.6727 - val_accuracy: 0.4512\n",
      "Epoch 35/100\n",
      "148/168 [=========================>....] - ETA: 0s - loss: 1.5273 - accuracy: 0.4922\n",
      "Epoch 35: val_loss did not improve from 1.66255\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5327 - accuracy: 0.4894 - val_loss: 1.6808 - val_accuracy: 0.4571\n",
      "Epoch 36/100\n",
      "144/168 [========================>.....] - ETA: 0s - loss: 1.5561 - accuracy: 0.4681\n",
      "Epoch 36: val_loss did not improve from 1.66255\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5578 - accuracy: 0.4692 - val_loss: 1.6739 - val_accuracy: 0.4676\n",
      "Epoch 37/100\n",
      "147/168 [=========================>....] - ETA: 0s - loss: 1.5318 - accuracy: 0.4838\n",
      "Epoch 37: val_loss improved from 1.66255 to 1.65617, saving model to D:\\Clg_Projects\\music_instruments(attempt2).hdf5\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5376 - accuracy: 0.4828 - val_loss: 1.6562 - val_accuracy: 0.4661\n",
      "Epoch 38/100\n",
      "162/168 [===========================>..] - ETA: 0s - loss: 1.5452 - accuracy: 0.4880\n",
      "Epoch 38: val_loss did not improve from 1.65617\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5439 - accuracy: 0.4894 - val_loss: 1.6817 - val_accuracy: 0.4579\n",
      "Epoch 39/100\n",
      "140/168 [========================>.....] - ETA: 0s - loss: 1.5457 - accuracy: 0.4830\n",
      "Epoch 39: val_loss did not improve from 1.65617\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5337 - accuracy: 0.4855 - val_loss: 1.6677 - val_accuracy: 0.4653\n",
      "Epoch 40/100\n",
      "140/168 [========================>.....] - ETA: 0s - loss: 1.5221 - accuracy: 0.4933\n",
      "Epoch 40: val_loss did not improve from 1.65617\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5151 - accuracy: 0.4909 - val_loss: 1.6772 - val_accuracy: 0.4564\n",
      "Epoch 41/100\n",
      "151/168 [=========================>....] - ETA: 0s - loss: 1.5136 - accuracy: 0.4932\n",
      "Epoch 41: val_loss did not improve from 1.65617\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5138 - accuracy: 0.4942 - val_loss: 1.6879 - val_accuracy: 0.4623\n",
      "Epoch 42/100\n",
      "144/168 [========================>.....] - ETA: 0s - loss: 1.5186 - accuracy: 0.4878\n",
      "Epoch 42: val_loss did not improve from 1.65617\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5166 - accuracy: 0.4918 - val_loss: 1.6593 - val_accuracy: 0.4586\n",
      "Epoch 43/100\n",
      "166/168 [============================>.] - ETA: 0s - loss: 1.5319 - accuracy: 0.4784\n",
      "Epoch 43: val_loss did not improve from 1.65617\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5320 - accuracy: 0.4787 - val_loss: 1.6663 - val_accuracy: 0.4750\n",
      "Epoch 44/100\n",
      "165/168 [============================>.] - ETA: 0s - loss: 1.5228 - accuracy: 0.4911\n",
      "Epoch 44: val_loss did not improve from 1.65617\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5245 - accuracy: 0.4914 - val_loss: 1.6815 - val_accuracy: 0.4601\n",
      "Epoch 45/100\n",
      "144/168 [========================>.....] - ETA: 0s - loss: 1.5080 - accuracy: 0.4980\n",
      "Epoch 45: val_loss did not improve from 1.65617\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5150 - accuracy: 0.4957 - val_loss: 1.6603 - val_accuracy: 0.4758\n",
      "Epoch 46/100\n",
      "146/168 [=========================>....] - ETA: 0s - loss: 1.5200 - accuracy: 0.4949\n",
      "Epoch 46: val_loss did not improve from 1.65617\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5179 - accuracy: 0.4957 - val_loss: 1.6752 - val_accuracy: 0.4780\n",
      "Epoch 47/100\n",
      "144/168 [========================>.....] - ETA: 0s - loss: 1.5234 - accuracy: 0.4831\n",
      "Epoch 47: val_loss did not improve from 1.65617\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5231 - accuracy: 0.4858 - val_loss: 1.6579 - val_accuracy: 0.4646\n",
      "Epoch 48/100\n",
      "167/168 [============================>.] - ETA: 0s - loss: 1.5011 - accuracy: 0.4901\n",
      "Epoch 48: val_loss did not improve from 1.65617\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5017 - accuracy: 0.4897 - val_loss: 1.6795 - val_accuracy: 0.4586\n",
      "Epoch 49/100\n",
      "150/168 [=========================>....] - ETA: 0s - loss: 1.5194 - accuracy: 0.4881\n",
      "Epoch 49: val_loss did not improve from 1.65617\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.5292 - accuracy: 0.4877 - val_loss: 1.6653 - val_accuracy: 0.4549\n",
      "Epoch 50/100\n",
      "149/168 [=========================>....] - ETA: 0s - loss: 1.5113 - accuracy: 0.4962\n",
      "Epoch 50: val_loss improved from 1.65617 to 1.64498, saving model to D:\\Clg_Projects\\music_instruments(attempt2).hdf5\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.5121 - accuracy: 0.4944 - val_loss: 1.6450 - val_accuracy: 0.4855\n",
      "Epoch 51/100\n",
      "163/168 [============================>.] - ETA: 0s - loss: 1.5176 - accuracy: 0.4927\n",
      "Epoch 51: val_loss did not improve from 1.64498\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 1.5166 - accuracy: 0.4931 - val_loss: 1.6788 - val_accuracy: 0.4743\n",
      "Epoch 52/100\n",
      "161/168 [===========================>..] - ETA: 0s - loss: 1.4744 - accuracy: 0.5033\n",
      "Epoch 52: val_loss improved from 1.64498 to 1.63695, saving model to D:\\Clg_Projects\\music_instruments(attempt2).hdf5\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.4719 - accuracy: 0.5026 - val_loss: 1.6370 - val_accuracy: 0.4840\n",
      "Epoch 53/100\n",
      "164/168 [============================>.] - ETA: 0s - loss: 1.5136 - accuracy: 0.4937\n",
      "Epoch 53: val_loss did not improve from 1.63695\n",
      "168/168 [==============================] - 2s 10ms/step - loss: 1.5116 - accuracy: 0.4944 - val_loss: 1.6473 - val_accuracy: 0.4556\n",
      "Epoch 54/100\n",
      "162/168 [===========================>..] - ETA: 0s - loss: 1.5041 - accuracy: 0.5031\n",
      "Epoch 54: val_loss improved from 1.63695 to 1.63256, saving model to D:\\Clg_Projects\\music_instruments(attempt2).hdf5\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 1.4990 - accuracy: 0.5054 - val_loss: 1.6326 - val_accuracy: 0.4802\n",
      "Epoch 55/100\n",
      "163/168 [============================>.] - ETA: 0s - loss: 1.4693 - accuracy: 0.5134\n",
      "Epoch 55: val_loss did not improve from 1.63256\n",
      "168/168 [==============================] - 2s 10ms/step - loss: 1.4720 - accuracy: 0.5134 - val_loss: 1.6380 - val_accuracy: 0.4765\n",
      "Epoch 56/100\n",
      "165/168 [============================>.] - ETA: 0s - loss: 1.5052 - accuracy: 0.4922\n",
      "Epoch 56: val_loss did not improve from 1.63256\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5053 - accuracy: 0.4914 - val_loss: 1.6376 - val_accuracy: 0.4780\n",
      "Epoch 57/100\n",
      "148/168 [=========================>....] - ETA: 0s - loss: 1.4796 - accuracy: 0.5046\n",
      "Epoch 57: val_loss did not improve from 1.63256\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4828 - accuracy: 0.5048 - val_loss: 1.6364 - val_accuracy: 0.4787\n",
      "Epoch 58/100\n",
      "150/168 [=========================>....] - ETA: 0s - loss: 1.4717 - accuracy: 0.5104\n",
      "Epoch 58: val_loss improved from 1.63256 to 1.61000, saving model to D:\\Clg_Projects\\music_instruments(attempt2).hdf5\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.4688 - accuracy: 0.5097 - val_loss: 1.6100 - val_accuracy: 0.4847\n",
      "Epoch 59/100\n",
      "149/168 [=========================>....] - ETA: 0s - loss: 1.4637 - accuracy: 0.5076\n",
      "Epoch 59: val_loss did not improve from 1.61000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 1.4727 - accuracy: 0.5047 - val_loss: 1.6427 - val_accuracy: 0.4795\n",
      "Epoch 60/100\n",
      "147/168 [=========================>....] - ETA: 0s - loss: 1.4796 - accuracy: 0.5060\n",
      "Epoch 60: val_loss did not improve from 1.61000\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4812 - accuracy: 0.5080 - val_loss: 1.6582 - val_accuracy: 0.4787\n",
      "Epoch 61/100\n",
      "151/168 [=========================>....] - ETA: 0s - loss: 1.4861 - accuracy: 0.5075\n",
      "Epoch 61: val_loss did not improve from 1.61000\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4857 - accuracy: 0.5069 - val_loss: 1.6374 - val_accuracy: 0.4847\n",
      "Epoch 62/100\n",
      "142/168 [========================>.....] - ETA: 0s - loss: 1.4783 - accuracy: 0.5064\n",
      "Epoch 62: val_loss did not improve from 1.61000\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4813 - accuracy: 0.5058 - val_loss: 1.6259 - val_accuracy: 0.4713\n",
      "Epoch 63/100\n",
      "142/168 [========================>.....] - ETA: 0s - loss: 1.4841 - accuracy: 0.5004\n",
      "Epoch 63: val_loss did not improve from 1.61000\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4838 - accuracy: 0.5024 - val_loss: 1.6368 - val_accuracy: 0.4884\n",
      "Epoch 64/100\n",
      "148/168 [=========================>....] - ETA: 0s - loss: 1.4801 - accuracy: 0.5051\n",
      "Epoch 64: val_loss did not improve from 1.61000\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4740 - accuracy: 0.5063 - val_loss: 1.6312 - val_accuracy: 0.4907\n",
      "Epoch 65/100\n",
      "145/168 [========================>.....] - ETA: 0s - loss: 1.4523 - accuracy: 0.5114\n",
      "Epoch 65: val_loss did not improve from 1.61000\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4638 - accuracy: 0.5089 - val_loss: 1.6203 - val_accuracy: 0.4795\n",
      "Epoch 66/100\n",
      "164/168 [============================>.] - ETA: 0s - loss: 1.4912 - accuracy: 0.5061\n",
      "Epoch 66: val_loss did not improve from 1.61000\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4876 - accuracy: 0.5069 - val_loss: 1.6415 - val_accuracy: 0.4810\n",
      "Epoch 67/100\n",
      "167/168 [============================>.] - ETA: 0s - loss: 1.4855 - accuracy: 0.5051\n",
      "Epoch 67: val_loss improved from 1.61000 to 1.59497, saving model to D:\\Clg_Projects\\music_instruments(attempt2).hdf5\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.4855 - accuracy: 0.5048 - val_loss: 1.5950 - val_accuracy: 0.4952\n",
      "Epoch 68/100\n",
      "157/168 [===========================>..] - ETA: 0s - loss: 1.4680 - accuracy: 0.5157\n",
      "Epoch 68: val_loss did not improve from 1.59497\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 1.4695 - accuracy: 0.5144 - val_loss: 1.6585 - val_accuracy: 0.4795\n",
      "Epoch 69/100\n",
      "145/168 [========================>.....] - ETA: 0s - loss: 1.4649 - accuracy: 0.5082\n",
      "Epoch 69: val_loss did not improve from 1.59497\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4646 - accuracy: 0.5112 - val_loss: 1.6373 - val_accuracy: 0.4825\n",
      "Epoch 70/100\n",
      "166/168 [============================>.] - ETA: 0s - loss: 1.4773 - accuracy: 0.5107\n",
      "Epoch 70: val_loss did not improve from 1.59497\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4781 - accuracy: 0.5110 - val_loss: 1.6331 - val_accuracy: 0.4705\n",
      "Epoch 71/100\n",
      "163/168 [============================>.] - ETA: 0s - loss: 1.4487 - accuracy: 0.5176\n",
      "Epoch 71: val_loss did not improve from 1.59497\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4528 - accuracy: 0.5168 - val_loss: 1.6225 - val_accuracy: 0.4698\n",
      "Epoch 72/100\n",
      "143/168 [========================>.....] - ETA: 0s - loss: 1.4616 - accuracy: 0.5122\n",
      "Epoch 72: val_loss did not improve from 1.59497\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4642 - accuracy: 0.5119 - val_loss: 1.6485 - val_accuracy: 0.4817\n",
      "Epoch 73/100\n",
      "143/168 [========================>.....] - ETA: 0s - loss: 1.4602 - accuracy: 0.5083\n",
      "Epoch 73: val_loss did not improve from 1.59497\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4652 - accuracy: 0.5067 - val_loss: 1.6403 - val_accuracy: 0.4676\n",
      "Epoch 74/100\n",
      "151/168 [=========================>....] - ETA: 0s - loss: 1.4786 - accuracy: 0.5106\n",
      "Epoch 74: val_loss did not improve from 1.59497\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4730 - accuracy: 0.5119 - val_loss: 1.6115 - val_accuracy: 0.4937\n",
      "Epoch 75/100\n",
      "141/168 [========================>.....] - ETA: 0s - loss: 1.4587 - accuracy: 0.5157\n",
      "Epoch 75: val_loss did not improve from 1.59497\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4672 - accuracy: 0.5157 - val_loss: 1.6234 - val_accuracy: 0.4862\n",
      "Epoch 76/100\n",
      "139/168 [=======================>......] - ETA: 0s - loss: 1.4388 - accuracy: 0.5180\n",
      "Epoch 76: val_loss did not improve from 1.59497\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4498 - accuracy: 0.5149 - val_loss: 1.6310 - val_accuracy: 0.4840\n",
      "Epoch 77/100\n",
      "147/168 [=========================>....] - ETA: 0s - loss: 1.4599 - accuracy: 0.5106\n",
      "Epoch 77: val_loss did not improve from 1.59497\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4586 - accuracy: 0.5106 - val_loss: 1.6321 - val_accuracy: 0.4840\n",
      "Epoch 78/100\n",
      "150/168 [=========================>....] - ETA: 0s - loss: 1.4683 - accuracy: 0.5160\n",
      "Epoch 78: val_loss did not improve from 1.59497\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4644 - accuracy: 0.5172 - val_loss: 1.6271 - val_accuracy: 0.4966\n",
      "Epoch 79/100\n",
      "147/168 [=========================>....] - ETA: 0s - loss: 1.4565 - accuracy: 0.5142\n",
      "Epoch 79: val_loss did not improve from 1.59497\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4493 - accuracy: 0.5140 - val_loss: 1.6098 - val_accuracy: 0.4907\n",
      "Epoch 80/100\n",
      "151/168 [=========================>....] - ETA: 0s - loss: 1.4667 - accuracy: 0.5120\n",
      "Epoch 80: val_loss did not improve from 1.59497\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4639 - accuracy: 0.5155 - val_loss: 1.5982 - val_accuracy: 0.4952\n",
      "Epoch 81/100\n",
      "166/168 [============================>.] - ETA: 0s - loss: 1.4383 - accuracy: 0.5173\n",
      "Epoch 81: val_loss improved from 1.59497 to 1.59394, saving model to D:\\Clg_Projects\\music_instruments(attempt2).hdf5\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.4367 - accuracy: 0.5181 - val_loss: 1.5939 - val_accuracy: 0.4698\n",
      "Epoch 82/100\n",
      "163/168 [============================>.] - ETA: 0s - loss: 1.4436 - accuracy: 0.5205\n",
      "Epoch 82: val_loss did not improve from 1.59394\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 1.4457 - accuracy: 0.5190 - val_loss: 1.6090 - val_accuracy: 0.4817\n",
      "Epoch 83/100\n",
      "166/168 [============================>.] - ETA: 0s - loss: 1.4415 - accuracy: 0.5284\n",
      "Epoch 83: val_loss did not improve from 1.59394\n",
      "168/168 [==============================] - 2s 10ms/step - loss: 1.4429 - accuracy: 0.5283 - val_loss: 1.5953 - val_accuracy: 0.4847\n",
      "Epoch 84/100\n",
      "165/168 [============================>.] - ETA: 0s - loss: 1.4502 - accuracy: 0.5102\n",
      "Epoch 84: val_loss did not improve from 1.59394\n",
      "168/168 [==============================] - 2s 10ms/step - loss: 1.4493 - accuracy: 0.5106 - val_loss: 1.6193 - val_accuracy: 0.4810\n",
      "Epoch 85/100\n",
      "147/168 [=========================>....] - ETA: 0s - loss: 1.4334 - accuracy: 0.5215\n",
      "Epoch 85: val_loss improved from 1.59394 to 1.59140, saving model to D:\\Clg_Projects\\music_instruments(attempt2).hdf5\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 1.4359 - accuracy: 0.5220 - val_loss: 1.5914 - val_accuracy: 0.5041\n",
      "Epoch 86/100\n",
      "156/168 [==========================>...] - ETA: 0s - loss: 1.4264 - accuracy: 0.5294\n",
      "Epoch 86: val_loss did not improve from 1.59140\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.4342 - accuracy: 0.5272 - val_loss: 1.6300 - val_accuracy: 0.4698\n",
      "Epoch 87/100\n",
      "161/168 [===========================>..] - ETA: 0s - loss: 1.4326 - accuracy: 0.5167\n",
      "Epoch 87: val_loss did not improve from 1.59140\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4292 - accuracy: 0.5186 - val_loss: 1.6003 - val_accuracy: 0.4855\n",
      "Epoch 88/100\n",
      "145/168 [========================>.....] - ETA: 0s - loss: 1.4200 - accuracy: 0.5263\n",
      "Epoch 88: val_loss did not improve from 1.59140\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4228 - accuracy: 0.5263 - val_loss: 1.6189 - val_accuracy: 0.5108\n",
      "Epoch 89/100\n",
      "145/168 [========================>.....] - ETA: 0s - loss: 1.4285 - accuracy: 0.5265\n",
      "Epoch 89: val_loss improved from 1.59140 to 1.57602, saving model to D:\\Clg_Projects\\music_instruments(attempt2).hdf5\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.4277 - accuracy: 0.5252 - val_loss: 1.5760 - val_accuracy: 0.5048\n",
      "Epoch 90/100\n",
      "142/168 [========================>.....] - ETA: 0s - loss: 1.4467 - accuracy: 0.5253\n",
      "Epoch 90: val_loss did not improve from 1.57602\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4628 - accuracy: 0.5220 - val_loss: 1.6474 - val_accuracy: 0.4735\n",
      "Epoch 91/100\n",
      "163/168 [============================>.] - ETA: 0s - loss: 1.4506 - accuracy: 0.5205\n",
      "Epoch 91: val_loss did not improve from 1.57602\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4463 - accuracy: 0.5220 - val_loss: 1.5964 - val_accuracy: 0.4959\n",
      "Epoch 92/100\n",
      "141/168 [========================>.....] - ETA: 0s - loss: 1.4492 - accuracy: 0.5164\n",
      "Epoch 92: val_loss did not improve from 1.57602\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4260 - accuracy: 0.5248 - val_loss: 1.6027 - val_accuracy: 0.4907\n",
      "Epoch 93/100\n",
      "152/168 [==========================>...] - ETA: 0s - loss: 1.4541 - accuracy: 0.5134\n",
      "Epoch 93: val_loss did not improve from 1.57602\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4555 - accuracy: 0.5123 - val_loss: 1.5961 - val_accuracy: 0.4922\n",
      "Epoch 94/100\n",
      "149/168 [=========================>....] - ETA: 0s - loss: 1.4570 - accuracy: 0.5197\n",
      "Epoch 94: val_loss did not improve from 1.57602\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4477 - accuracy: 0.5268 - val_loss: 1.5920 - val_accuracy: 0.4914\n",
      "Epoch 95/100\n",
      "146/168 [=========================>....] - ETA: 0s - loss: 1.4367 - accuracy: 0.5246\n",
      "Epoch 95: val_loss did not improve from 1.57602\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4349 - accuracy: 0.5239 - val_loss: 1.5919 - val_accuracy: 0.5071\n",
      "Epoch 96/100\n",
      "146/168 [=========================>....] - ETA: 0s - loss: 1.4308 - accuracy: 0.5216\n",
      "Epoch 96: val_loss did not improve from 1.57602\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4248 - accuracy: 0.5254 - val_loss: 1.5947 - val_accuracy: 0.4862\n",
      "Epoch 97/100\n",
      "140/168 [========================>.....] - ETA: 0s - loss: 1.4273 - accuracy: 0.5234\n",
      "Epoch 97: val_loss did not improve from 1.57602\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4323 - accuracy: 0.5226 - val_loss: 1.5995 - val_accuracy: 0.5108\n",
      "Epoch 98/100\n",
      "144/168 [========================>.....] - ETA: 0s - loss: 1.4217 - accuracy: 0.5323\n",
      "Epoch 98: val_loss did not improve from 1.57602\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4302 - accuracy: 0.5309 - val_loss: 1.5997 - val_accuracy: 0.5048\n",
      "Epoch 99/100\n",
      "143/168 [========================>.....] - ETA: 0s - loss: 1.4000 - accuracy: 0.5372\n",
      "Epoch 99: val_loss did not improve from 1.57602\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.3962 - accuracy: 0.5386 - val_loss: 1.5933 - val_accuracy: 0.5019\n",
      "Epoch 100/100\n",
      "168/168 [==============================] - ETA: 0s - loss: 1.4272 - accuracy: 0.5203\n",
      "Epoch 100: val_loss did not improve from 1.57602\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 1.4272 - accuracy: 0.5203 - val_loss: 1.5970 - val_accuracy: 0.4892\n",
      "Training completed in time:  0:00:51.074850\n"
     ]
    }
   ],
   "source": [
    "#TRAINING A MODEL(ATTEMPT 2)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "\n",
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath = 'D:\\Clg_Projects\\music_instruments(attempt2).hdf5',verbose = 1, save_best_only = True)\n",
    "\n",
    "start = datetime.now()\n",
    "model.fit(x_train,y_train,batch_size = num_batch_size,epochs = num_epochs, validation_data = (x_test,y_test),callbacks = [checkpointer])\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "45f60033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.1344073e+02,  1.3957828e+02, -3.2532764e+01, ...,\n",
       "        -2.4461564e-01, -5.1348501e-01, -1.7173027e+01],\n",
       "       [-3.6271506e+02,  1.3869600e+02, -6.3733017e+01, ...,\n",
       "         4.2085199e+00,  1.8003825e+00, -5.7004869e-01],\n",
       "       [-1.8562082e+02,  9.6106987e+01, -6.0718140e+01, ...,\n",
       "        -9.4017908e-02, -7.3232036e+00, -2.4550588e+00],\n",
       "       ...,\n",
       "       [-1.5897318e+02,  9.0076683e+01, -2.6815508e+01, ...,\n",
       "         2.3733896e-01, -8.9178152e+00, -2.2492950e+00],\n",
       "       [-4.5192392e+02,  1.2029269e+02, -2.6154728e+01, ...,\n",
       "         5.7742944e+00,  8.9114523e+00, -6.4587512e+00],\n",
       "       [-2.0438811e+02,  1.0340012e+02, -1.3882885e+01, ...,\n",
       "        -1.2050216e+01, -6.8083439e+00, -2.3715210e+00]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "231ac9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "74d281ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMALIZING THE INPUT FEATURES\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6bfe146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the instrument labels to one-hot encoded vectors\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "28dbc793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        ...,\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c3ce39d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\anann\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\anann\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\anann\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\anann\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1055, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\anann\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1149, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\Users\\anann\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 605, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"C:\\Users\\anann\\anaconda3\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 77, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"C:\\Users\\anann\\anaconda3\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"C:\\Users\\anann\\anaconda3\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 691, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\anann\\anaconda3\\lib\\site-packages\\keras\\metrics\\accuracy_metrics.py\", line 426, in categorical_accuracy\n        return metrics_utils.sparse_categorical_matches(\n    File \"C:\\Users\\anann\\anaconda3\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 963, in sparse_categorical_matches\n        y_true = tf.squeeze(y_true, [-1])\n\n    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 11 for '{{node Squeeze}} = Squeeze[T=DT_INT64, squeeze_dims=[-1]](ArgMax)' with input shapes: [?,11].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m checkpointer \u001b[38;5;241m=\u001b[39m ModelCheckpoint(filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mClg_Projects\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmusic_instruments(attempt3).hdf5\u001b[39m\u001b[38;5;124m'\u001b[39m,verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, save_best_only \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m start \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m duration \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed in time: \u001b[39m\u001b[38;5;124m\"\u001b[39m, duration)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file9aaulm2s.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\anann\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\anann\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\anann\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\anann\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1055, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\anann\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1149, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\Users\\anann\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 605, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"C:\\Users\\anann\\anaconda3\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 77, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"C:\\Users\\anann\\anaconda3\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"C:\\Users\\anann\\anaconda3\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 691, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\anann\\anaconda3\\lib\\site-packages\\keras\\metrics\\accuracy_metrics.py\", line 426, in categorical_accuracy\n        return metrics_utils.sparse_categorical_matches(\n    File \"C:\\Users\\anann\\anaconda3\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 963, in sparse_categorical_matches\n        y_true = tf.squeeze(y_true, [-1])\n\n    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 11 for '{{node Squeeze}} = Squeeze[T=DT_INT64, squeeze_dims=[-1]](ArgMax)' with input shapes: [?,11].\n"
     ]
    }
   ],
   "source": [
    "#TRAINING A MODEL(ATTEMPT 4)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "\n",
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath = 'D:\\Clg_Projects\\music_instruments(attempt3).hdf5',verbose = 1, save_best_only = True)\n",
    "\n",
    "start = datetime.now()\n",
    "model.fit(x_train,y_train,batch_size = num_batch_size,epochs = num_epochs, validation_data = (x_test,y_test),callbacks = [checkpointer])\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "11a9b835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5364, 40)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1ef2f501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5364, 11, 2)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "326d9779",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef17cf1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
